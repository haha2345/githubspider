{
    "repo_name": "immich",
    "watchers": "157",
    "forks": "2.3k",
    "stars": "46.3k",
    "languages": {},
    "about_info": "High performance self-hosted photo and video management solution.",
    "repo_url": "https://github.com/immich-app/immich",
    "readme_content": "High performance self-hosted photo and video management solution\n\n\n\n\n\n\nCatalà\nEspañol\nFrançais\nItaliano\n日本語\n한국어\nDeutsch\nNederlands\nTürkçe\n中文\nРусский\nPortuguês Brasileiro\nSvenska\nالعربية\nTiếng Việt\n\nDisclaimer\n\n⚠️ The project is under very active development.\n⚠️ Expect bugs and breaking changes.\n⚠️ Do not use the app as the only way to store your photos and videos.\n⚠️ Always follow 3-2-1 backup plan for your precious photos and videos!\n\nNoteYou can find the main documentation, including installation guides, at https://immich.app/.\n\nLinks\n\nDocumentation\nAbout\nInstallation\nRoadmap\nDemo\nFeatures\nTranslations\nContributing\n\nDemo\nAccess the demo here. The demo is running on a Free-tier Oracle VM in Amsterdam with a 2.4Ghz quad-core ARM64 CPU and 24GB RAM.\nFor the mobile app, you can use https://demo.immich.app/api for the Server Endpoint URL\nLogin credentials\n\n\n\nEmail\nPassword\n\n\n\n\ndemo@immich.app\ndemo\n\n\n\nFeatures\n\n\n\nFeatures\nMobile\nWeb\n\n\n\n\nUpload and view videos and photos\nYes\nYes\n\n\nAuto backup when the app is opened\nYes\nN/A\n\n\nPrevent duplication of assets\nYes\nYes\n\n\nSelective album(s) for backup\nYes\nN/A\n\n\nDownload photos and videos to local device\nYes\nYes\n\n\nMulti-user support\nYes\nYes\n\n\nAlbum and Shared albums\nYes\nYes\n\n\nScrubbable/draggable scrollbar\nYes\nYes\n\n\nSupport raw formats\nYes\nYes\n\n\nMetadata view (EXIF, map)\nYes\nYes\n\n\nSearch by metadata, objects, faces, and CLIP\nYes\nYes\n\n\nAdministrative functions (user management)\nNo\nYes\n\n\nBackground backup\nYes\nN/A\n\n\nVirtual scroll\nYes\nYes\n\n\nOAuth support\nYes\nYes\n\n\nAPI Keys\nN/A\nYes\n\n\nLivePhoto/MotionPhoto backup and playback\nYes\nYes\n\n\nSupport 360 degree image display\nNo\nYes\n\n\nUser-defined storage structure\nYes\nYes\n\n\nPublic Sharing\nYes\nYes\n\n\nArchive and Favorites\nYes\nYes\n\n\nGlobal Map\nYes\nYes\n\n\nPartner Sharing\nYes\nYes\n\n\nFacial recognition and clustering\nYes\nYes\n\n\nMemories (x years ago)\nYes\nYes\n\n\nOffline support\nYes\nNo\n\n\nRead-only gallery\nYes\nYes\n\n\nStacked Photos\nYes\nYes\n\n\n\nTranslations\nRead more about translations here.\n\n\n\nRepository activity\n\nStar history\n\n \n   \n   \n   \n \n\nContributors"
}
{
    "repo_name": "Retrieval-based-Voice-Conversion-WebUI",
    "watchers": "172",
    "forks": "3.5k",
    "stars": "23.3k",
    "languages": {},
    "about_info": "Easily train a good VC model with voice data <= 10 mins!",
    "repo_url": "https://github.com/RVC-Project/Retrieval-based-Voice-Conversion-WebUI",
    "readme_content": "Retrieval-based-Voice-Conversion-WebUI\n一个基于VITS的简单易用的变声框架\n\n\n\n\n\n\n更新日志 | 常见问题解答 | AutoDL·5毛钱训练AI歌手 | 对照实验记录 | 在线演示\nEnglish | 中文简体 | 日本語 | 한국어 (韓國語) | Français | Türkçe | Português\n\n\n底模使用接近50小时的开源高质量VCTK训练集训练，无版权方面的顾虑，请大家放心使用\n\n\n请期待RVCv3的底模，参数更大，数据更大，效果更好，基本持平的推理速度，需要训练数据量更少。\n\n\n   \n\t\t训练推理界面\n\t\t实时变声界面\n\t\n  \n\t\t\n    \n\t\n\t\n\t\tgo-web.bat\n\t\tgo-realtime-gui.bat\n\t\n  \n    可以自由选择想要执行的操作。\n\t\t我们已经实现端到端170ms延迟。如使用ASIO输入输出设备，已能实现端到端90ms延迟，但非常依赖硬件驱动支持。\n\t\n\n简介\n本仓库具有以下特点\n\n使用top1检索替换输入源特征为训练集特征来杜绝音色泄漏\n即便在相对较差的显卡上也能快速训练\n使用少量数据进行训练也能得到较好结果(推荐至少收集10分钟低底噪语音数据)\n可以通过模型融合来改变音色(借助ckpt处理选项卡中的ckpt-merge)\n简单易用的网页界面\n可调用UVR5模型来快速分离人声和伴奏\n使用最先进的人声音高提取算法InterSpeech2023-RMVPE根绝哑音问题。效果最好（显著地）但比crepe_full更快、资源占用更小\nA卡I卡加速支持\n\n点此查看我们的演示视频 !\n环境配置\n以下指令需在 Python 版本大于3.8的环境中执行。\nWindows/Linux/MacOS等平台通用方法\n下列方法任选其一。\n1. 通过 pip 安装依赖\n\n安装Pytorch及其核心依赖，若已安装则跳过。参考自: https://pytorch.org/get-started/locally/\n\npip install torch torchvision torchaudio\n\n如果是 win 系统 + Nvidia Ampere 架构(RTX30xx)，根据 #21 的经验，需要指定 pytorch 对应的 cuda 版本\n\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n\n根据自己的显卡安装对应依赖\n\n\nN卡\n\npip install -r requirements.txt\n\nA卡/I卡\n\npip install -r requirements-dml.txt\n\nA卡ROCM(Linux)\n\npip install -r requirements-amd.txt\n\nI卡IPEX(Linux)\n\npip install -r requirements-ipex.txt\n2. 通过 poetry 来安装依赖\n安装 Poetry 依赖管理工具，若已安装则跳过。参考自: https://python-poetry.org/docs/#installation\ncurl -sSL https://install.python-poetry.org | python3 -\n通过 Poetry 安装依赖时，python 建议使用 3.7-3.10 版本，其余版本在安装 llvmlite==0.39.0 时会出现冲突\npoetry init -n\npoetry env use \"path to your python.exe\"\npoetry run pip install -r requirments.txt\nMacOS\n可以通过 run.sh 来安装依赖\nsh ./run.sh\n其他预模型准备\nRVC需要其他一些预模型来推理和训练。\n你可以从我们的Hugging Face space下载到这些模型。\n1. 下载 assets\n以下是一份清单，包括了所有RVC所需的预模型和其他文件的名称。你可以在tools文件夹找到下载它们的脚本。\n\n\n./assets/hubert/hubert_base.pt\n\n\n./assets/pretrained\n\n\n./assets/uvr5_weights\n\n\n想使用v2版本模型的话，需要额外下载\n\n./assets/pretrained_v2\n\n2. 安装 ffmpeg\n若ffmpeg和ffprobe已安装则跳过。\nUbuntu/Debian 用户\nsudo apt install ffmpeg\nMacOS 用户\nbrew install ffmpeg\nWindows 用户\n下载后放置在根目录。\n\n\n下载ffmpeg.exe\n\n\n下载ffprobe.exe\n\n\n3. 下载 rmvpe 人声音高提取算法所需文件\n如果你想使用最新的RMVPE人声音高提取算法，则你需要下载音高提取模型参数并放置于RVC根目录。\n\n下载rmvpe.pt\n\n下载 rmvpe 的 dml 环境(可选, A卡/I卡用户)\n\n下载rmvpe.onnx\n\n4. AMD显卡Rocm(可选, 仅Linux)\n如果你想基于AMD的Rocm技术在Linux系统上运行RVC，请先在这里安装所需的驱动。\n若你使用的是Arch Linux，可以使用pacman来安装所需驱动：\npacman -S rocm-hip-sdk rocm-opencl-sdk\n\n对于某些型号的显卡，你可能需要额外配置如下的环境变量（如：RX6700XT）：\nexport ROCM_PATH=/opt/rocm\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\n\n同时确保你的当前用户处于render与video用户组内：\nsudo usermod -aG render $USERNAME\nsudo usermod -aG video $USERNAME\n\n开始使用\n直接启动\n使用以下指令来启动 WebUI\npython infer-web.py\n若先前使用 Poetry 安装依赖，则可以通过以下方式启动WebUI\npoetry run python infer-web.py\n使用整合包\n下载并解压RVC-beta.7z\nWindows 用户\n双击go-web.bat\nMacOS 用户\nsh ./run.sh\n对于需要使用IPEX技术的I卡用户(仅Linux)\nsource /opt/intel/oneapi/setvars.sh\n参考项目\n\nContentVec\nVITS\nHIFIGAN\nGradio\nFFmpeg\nUltimate Vocal Remover\naudio-slicer\nVocal pitch extraction:RMVPE\n\nThe pretrained model is trained and tested by yxlllc and RVC-Boss.\n\n\n\n感谢所有贡献者作出的努力"
}
{
    "repo_name": "dice",
    "watchers": "30",
    "forks": "662",
    "stars": "5.2k",
    "languages": {},
    "about_info": "DiceDB is an in-memory, real-time, and reactive database with Redis and SQL support optimized for modern hardware and building real-time applications.",
    "repo_url": "https://github.com/DiceDB/dice",
    "readme_content": "DiceDB\nDiceDB is an in-memory, real-time, and reactive database with Redis and SQL support optimized for modern hardware and building real-time applications.\nWe are looking for Early Design Partners, so, if you want to evaluate DiceDB, block our calendar. always up for a chat.\n\nNote: DiceDB is still in development and it supports a subset of Redis commands. So, please do not use it in production. But, feel free to go through the open issues and contribute to help us speed up the development.\n\nWant to contribute?\nWe have multiple repositories where you can contribute. So, as per your interest, you can pick one and build a deeper understanding of the project on the go.\n\ndicedb/dice for core database features and engine / Stack - Go\ndicedb/playground-mono backend APIs for DiceDB playground / Stack - Go\ndicedb/playground-web frontend for DiceDB playground / Stack - NextJS\n\nHow is it different from Redis?\nAlthough DiceDB is a drop-in replacement of Redis, which means almost no learning curve and switching does not require any code change, it still differs in two key aspects and they are\n\nDiceDB is multithreaded and follows shared-nothing architecture.\nDiceDB supports a new command called QWATCH that lets clients listen to a SQL query and get notified in real-time whenever something changes.\n\nWith this, you can build truly real-time applications like Leaderboard with simple SQL query.\n\nGet started\nUsing Docker\nThe easiest way to get started with DiceDB is using Docker by running the following command.\n$ docker run dicedb/dicedb\n\nThe above command will start the DiceDB server running locally on the port 7379 and you can connect\nto it using DiceDB CLI and SDKs, or even Redis CLIs and SDKs.\n\nNote: Given it is a drop-in replacement of Redis, you can also use any Redis CLI and SDK to connect to DiceDB.\n\nMulti-Threading Mode (Experimental)\nMulti-threading is currently under active development. To run the server with multi-threading enabled, follow these steps:\n$ git clone https://github.com/dicedb/dice\n$ cd dice\n$ go run main.go --enable-multithreading=true\nNote: Only the following commands are optimised for multithreaded execution: PING, AUTH, SET, GET, GETSET, ABORT\nSetting up DiceDB from source for development and contributions\nTo run DiceDB for local development or running from source, you will need\n\nGolang\nAny of the below supported platform environment:\n\nLinux based environment\nOSX (Darwin) based environment\nWSL under Windows\n\n\n\n$ git clone https://github.com/dicedb/dice\n$ cd dice\n$ go run main.go\n\n\nInstall GoLangCI\n\n$ sudo su\n$ curl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b /bin v1.60.1\n\nLive Development Server\nDiceDB provides a hot-reloading development environment, which allows you to instantly view your code changes in a live server. This functionality is supported by Air\nTo Install Air on your system you have the following options.\n\nIf you're on go 1.22+\n\ngo install github.com/air-verse/air@latest\n\nInstall the Air binary\n\n# binary will be installed at $(go env GOPATH)/bin/air\ncurl -sSfL https://raw.githubusercontent.com/air-verse/air/master/install.sh | sh -s -- -b $(go env GOPATH)/bin\nOnce air is installed you can verify the installation using the command air -v\nTo run the live DiceDB server for local development:\n$ git clone https://github.com/dicedb/dice\n$ cd dice\n$ DICE_ENV=dev air\n\nThe DICE_ENV environment variable is used set the environment, by default it is treated as production. dev is used to get pretty printed logs and lower log level.\n\nLocal Setup with Custom Config\nBy default, DiceDB will look for the configuration file at /etc/dice/config.toml. (Linux, Darwin, and WSL)\n$ # set up configuration file # (optional but recommended)\n$ sudo mkdir -p /etc/dice\n$ sudo chown root:$USER /etc/dice\n$ sudo chmod 775 /etc/dice # or 777 if you are the only user\n$ git clone https://github.com/DiceDB/dice.git\n$ cd dice\n$ go run main.go -init-config\n\nFor Windows Users:\nIf you're using Windows, it is recommended to use Windows Subsystem for Linux (WSL) or WSL 2 to run the above commands seamlessly in a Linux-like environment.\nAlternatively, you can:\nCreate a directory at C:\\ProgramData\\dice and run the following command to generate the configuration file:\ngo run main.go -init-config\nFor a smoother experience, we highly recommend using WSL.\nAdditional Configuration Options:\nIf you'd like to use a different location, you can specify a custom configuration file path with the -c flag:\ngo run main.go -c /path/to/config.toml\nIf you'd like to output the configuration file to a specific location, you can specify a custom output path with the -o flag:\ngo run main.go -o /path/of/output/dir\nSetting up CLI\nThe best way to connect to DiceDB is using DiceDB CLI and you can install it by running the following command.\n$ pip install dicedb-cli\n\n\nBecause DiceDB speaks Redis dialect, you can connect to it with any Redis Client and SDK also.\nBut if you are planning to use the QWATCH feature then you need to use the DiceDB CLI.\n\nRunning Tests\nUnit tests and integration tests are essential for ensuring correctness and in the case of DiceDB, both types of tests are available to validate its functionality.\nFor unit testing, you can execute individual unit tests by specifying the name of the test function using the TEST_FUNC environment variable and running the make unittest-one command. Alternatively, running make unittest will execute all unit tests.\nExecuting one unit test\n$ TEST_FUNC=<name of the test function> make unittest-one\n$ TEST_FUNC=TestByteList make unittest-one\n\nRunning all unit tests\n$ make unittest\n\nIntegration tests, on the other hand, involve starting up the DiceDB server and running a series of commands to verify the expected end state and output. To execute a single integration test, you can set the TEST_FUNC environment variable to the name of the test function and run make test-one. Running make test will execute all integration tests.\nExecuting a single integration test\n$ TEST_FUNC=<name of the test function> make test-one\n$ TEST_FUNC=TestSet make test-one\n\nRunning all integration tests\n$ make test\n\n\nWork to add more tests in DiceDB is in progress, and we will soon port the\ntest Redis suite to this codebase to ensure full compatibility.\n\nRunning Benchmark\n$ go test -test.bench <pattern>\n$ go test -test.bench BenchmarkListRedis -benchmem\nGetting Started\nTo get started with building and contributing to DiceDB, please refer to the issues created in this repository.\nDocs\n\n\nWe use Astro framework to power the dicedb.io website and Starlight to power the docs. Once you have NodeJS installed, fire the following commands to get your local version of dicedb.io running.\n$ cd docs\n$ npm install\n$ npm run dev\n\nOnce the server starts, visit http://localhost:4321/ in your favourite browser. This runs with a hot reload which means any changes you make in the website and the documentation can be instantly viewed on the browser.\nTo build and deploy\n$ cd docs\n$ npm run build\n\nDocs directory structure\n\ndocs/src/content/docs/commands is where all the commands are documented\ndocs/src/content/docs/tutorials is where all the tutorials are documented\n\nThe story\nDiceDB started as a re-implementation of Redis in Golang and the idea was to - build a DB from scratch and understand the micro-nuances that come with its implementation. The database does not aim to replace Redis, instead, it will fit in and optimize itself for multicore computations running on a single-threaded event loop.\nHow to contribute\nThe Code Contribution Guidelines are published at CONTRIBUTING.md; please read them before you start making any changes. This would allow us to have a consistent standard of coding practices and developer experience.\nContributors can join the Discord Server for quick collaboration.\nContributors\n\n  \n\nTroubleshoot\nForcefully killing the process\n$ sudo netstat -atlpn | grep :7379\n$ sudo kill -9 <process_id>"
}
{
    "repo_name": "nuttx",
    "watchers": "97",
    "forks": "1.1k",
    "stars": "2.7k",
    "languages": {},
    "about_info": "Apache NuttX is a mature, real-time embedded operating system (RTOS)",
    "repo_url": "https://github.com/apache/nuttx",
    "readme_content": "Apache NuttX is a real-time operating system (RTOS) with an emphasis on\nstandards compliance and small footprint. Scalable from 8-bit to 64-bit\nmicrocontroller environments, the primary governing standards in NuttX are POSIX\nand ANSI standards. Additional standard APIs from Unix and other common RTOSs\n(such as VxWorks) are adopted for functionality not available under these\nstandards, or for functionality that is not appropriate for deeply-embedded\nenvironments (such as fork()).\nFor brevity, many parts of the documentation will refer to Apache NuttX as simply NuttX.\nGetting Started\nFirst time on NuttX? Read the Getting Started guide!\nIf you don't have a board available, NuttX has its own simulator that you can run on terminal.\nDocumentation\nYou can find the current NuttX documentation on the Documentation Page.\nAlternatively, you can build the documentation yourself by following the Documentation Build Instructions.\nThe old NuttX documentation is still available in the Apache wiki.\nSupported Boards\nNuttX supports a wide variety of platforms. See the full list on the Supported Platforms page.\nContributing\nIf you wish to contribute to the NuttX project, read the Contributing guidelines for information on Git usage, coding standard, workflow and the NuttX principles.\nLicense\nThe code in this repository is under either the Apache 2 license, or a license compatible with the Apache 2 license. See the License Page for more information."
}
{
    "repo_name": "screenpipe",
    "watchers": "20",
    "forks": "126",
    "stars": "2.3k",
    "languages": {},
    "about_info": "24/7 local AI screen & mic recording. Build AI apps that have the full context. Works with Ollama. Alternative to Rewind.ai. Open. Secure. You own your data. Rust.",
    "repo_url": "https://github.com/mediar-ai/screenpipe",
    "readme_content": "___  ___ _ __ ___  ___ _ __  _ __ (_)_ __   ___ \n  / __|/ __| '__/ _ \\/ _ \\ '_ \\| '_ \\| | '_ \\ / _ \\\n  \\__ \\ (__| | |  __/  __/ | | | |_) | | |_) |  __/\n  |___/\\___|_|  \\___|\\___|_| |_| .__/|_| .__/ \\___|\n                               |_|     |_|         \n\n\n    \n        \n    \n\n\n    \n        \n    \n\n\n    \n        \n    \n        \n\n\n  \n    \n  \n\n   \n       \n   \n\n\nLatest News 🔥\n\n[2024/09] 70 users run screenpipe 24/7!\n[2024/09] Released a v0 of our documentation\n[2024/08] Anyone can now create, share, install pipes (plugins) from the app interface based on a github repo/dir\n[2024/08] We're running bounties! Contribute to screenpipe & make money, check issues\n[2024/08] Audio input & output now works perfect on Windows, Linux, MacOS (<15.0). We also support multi monitor capture and defaulting STT to Whisper Distil large v3\n[2024/08] We released video embedding. AI gives you links to your video recording in the chat!\n[2024/08] We released the pipe store! Create, share, use plugins that get you the most out of your data in less than 30s, even if you are not technical.\n[2024/08] We released Apple & Windows Native OCR.\n[2024/08] The Linux desktop app is here!.\n[2024/07] The Windows desktop app is here! Get it now!.\n[2024/07] 🎁 Screenpipe won Friends (the AI necklace) hackathon at AGI House (integrations soon)\n[2024/07] We just launched the desktop app! Download now!\n\n\n24/7 Screen & Audio Capture\nLibrary to build personalized AI powered by what you've seen, said, or heard. Works with Ollama. Alternative to Rewind.ai. Open. Secure. You own your data. Rust.\nWe are shipping daily, make suggestions, post bugs, give feedback.\n\nWhy?\nBuilding a reliable stream of audio and screenshot data, where a user simply clicks a button and the script runs in the background 24/7, collecting and extracting data from screen and audio input/output, can be frustrating.\nThere are numerous use cases that can be built on top of this layer. To simplify life for other developers, we decided to solve this non-trivial problem. It's still in its early stages, but it works end-to-end. We're working on this full-time and would love to hear your feedback and suggestions.\nGet started\nThere are multiple ways to install screenpipe:\n\nas a CLI for technical users\nas a paid desktop app with 1 year updates, priority support, and priority features\nas a free forever desktop app (but you need to build it yourself). We're 100% OSS.\nas a free forever desktop app - by sending a PR (example) or sharing about screenpipe online\nas a Rust or WASM library - check this websocket to stream frames + OCR to your app\nas a business\n\n👉 install screenpipe now\nusage\nscreenpipe has a plugin system called \"pipe\" which lets you run code in a sandboxed environment within the Rust code, get started\nexamples\ncheck examples\nstar history\n\nContributing\nContributions are welcome! If you'd like to contribute, please read CONTRIBUTING.md."
}
{
    "repo_name": "TTS",
    "watchers": "284",
    "forks": "4.1k",
    "stars": "33.9k",
    "languages": {},
    "about_info": "🐸💬 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
    "repo_url": "https://github.com/coqui-ai/TTS",
    "readme_content": "🐸Coqui.ai News\n\n📣 ⓍTTSv2 is here with 16 languages and better performance across the board.\n📣 ⓍTTS fine-tuning code is out. Check the example recipes.\n📣 ⓍTTS can now stream with <200ms latency.\n📣 ⓍTTS, our production TTS model that can speak 13 languages, is released Blog Post, Demo, Docs\n📣 🐶Bark is now available for inference with unconstrained voice cloning. Docs\n📣 You can use ~1100 Fairseq models with 🐸TTS.\n📣 🐸TTS now supports 🐢Tortoise with faster inference. Docs\n\n\n\n\n🐸TTS is a library for advanced Text-to-Speech generation.\n🚀 Pretrained models in +1100 languages.\n🛠️ Tools for training new models and fine-tuning existing models in any language.\n📚 Utilities for dataset analysis and curation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n💬 Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n\n\nType\nPlatforms\n\n\n\n\n🚨 Bug Reports\nGitHub Issue Tracker\n\n\n🎁 Feature Requests & Ideas\nGitHub Issue Tracker\n\n\n👩‍💻 Usage Questions\nGitHub Discussions\n\n\n🗯 General Discussion\nGitHub Discussions or Discord\n\n\n\n🔗 Links and Resources\n\n\n\nType\nLinks\n\n\n\n\n💼 Documentation\nReadTheDocs\n\n\n💾 Installation\nTTS/README.md\n\n\n👩‍💻 Contributing\nCONTRIBUTING.md\n\n\n📌 Road Map\nMain Development Plans\n\n\n🚀 Released Models\nTTS Releases and Experimental Models\n\n\n📰 Papers\nTTS Papers\n\n\n\n🥇 TTS Performance\n\nUnderlined \"TTS*\" and \"Judy*\" are internal 🐸TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\nFeatures\n\nHigh-performance Deep Learning models for Text2Speech tasks.\n\nText2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\nSpeaker Encoder to compute speaker embeddings efficiently.\nVocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n\n\nFast and efficient model training.\nDetailed training logs on the terminal and Tensorboard.\nSupport for Multi-speaker TTS.\nEfficient, flexible, lightweight but feature complete Trainer API.\nReleased and ready-to-use models.\nTools to curate Text2Speech datasets underdataset_analysis.\nUtilities to use and test your models.\nModular (but not too much) code base enabling easy implementation of new ideas.\n\nModel Implementations\nSpectrogram models\n\nTacotron: paper\nTacotron2: paper\nGlow-TTS: paper\nSpeedy-Speech: paper\nAlign-TTS: paper\nFastPitch: paper\nFastSpeech: paper\nFastSpeech2: paper\nSC-GlowTTS: paper\nCapacitron: paper\nOverFlow: paper\nNeural HMM TTS: paper\nDelightful TTS: paper\n\nEnd-to-End Models\n\nⓍTTS: blog\nVITS: paper\n🐸 YourTTS: paper\n🐢 Tortoise: orig. repo\n🐶 Bark: orig. repo\n\nAttention Methods\n\nGuided Attention: paper\nForward Backward Decoding: paper\nGraves Attention: paper\nDouble Decoder Consistency: blog\nDynamic Convolutional Attention: paper\nAlignment Network: paper\n\nSpeaker Encoder\n\nGE2E: paper\nAngular Loss: paper\n\nVocoders\n\nMelGAN: paper\nMultiBandMelGAN: paper\nParallelWaveGAN: paper\nGAN-TTS discriminators: paper\nWaveRNN: origin\nWaveGrad: paper\nHiFiGAN: paper\nUnivNet: paper\n\nVoice Conversion\n\nFreeVC: paper\n\nYou can also help us implement more models.\nInstallation\n🐸TTS is tested on Ubuntu 18.04 with python >= 3.9, < 3.12..\nIf you are only interested in synthesizing speech with the released 🐸TTS models, installing from PyPI is the easiest option.\npip install TTS\nIf you plan to code or train models, clone 🐸TTS and install it locally.\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\nIf you are on Windows, 👑@GuyPaddock wrote installation instructions here.\nDocker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\nYou can then enjoy the TTS server here\nMore details about the docker images (like GPU support) can be found here\nSynthesizing speech by 🐸TTS\n🐍 Python API\nRunning a multi-speaker and multi-lingual model\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# List available 🐸TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# Run TTS\n# ❗ Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# Text to speech to a file\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\nRunning a single speaker model\n# Init TTS with the target model name\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso é clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\nExample voice conversion\nConverting the voice in source_wav to the voice of target_wav\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\nExample voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in 🐸TTS.\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\ntts.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\nExample text to speech using Fairseq models in ~1100 languages 🤯.\nFor Fairseq models, use the following name format: tts_models/<lang-iso_code>/fairseq/vits.\nYou can find the language ISO codes here\nand learn about the Fairseq models here.\n# TTS with on the fly voice conversion\napi = TTS(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\nCommand-line tts\n\nSynthesize speech on command line.\nYou can either use your trained model or choose a model from the provided list.\nIf you don't specify any models, then it uses LJSpeech based English model.\nSingle Speaker Models\n\n\nList provided models:\n$ tts --list_models\n\n\n\nGet model info (for both tts_models and vocoder_models):\n\n\nQuery by type/name:\nThe model_info_by_name uses the name as it from the --list_models.\n$ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n\nFor example:\n$ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n$ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n\n\n\nQuery by type/idx:\nThe model_query_idx uses the corresponding idx from --list_models.\n$ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n\nFor example:\n$ tts --model_info_by_idx tts_models/3\n\n\n\nQuery info for model info by full name:\n$ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n\n\n\n\n\nRun TTS with default models:\n$ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n\n\n\nRun TTS and pipe out the generated TTS wav file data:\n$ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n\n\n\nRun a TTS model with its default vocoder model:\n$ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n\nFor example:\n$ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n\n\n\nRun with specific TTS and vocoder models from the list:\n$ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n\nFor example:\n$ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n\n\n\nRun your own TTS model (Using Griffin-Lim Vocoder):\n$ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n\n\n\nRun your own TTS and Vocoder models:\n$ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n    --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n\n\n\nMulti-speaker Models\n\n\nList the available speakers and choose a <speaker_id> among them:\n$ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n\n\n\nRun the multi-speaker TTS model with the target speaker ID:\n$ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n\n\n\nRun your own multi-speaker TTS model:\n$ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n\n\n\nVoice Conversion Models\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n\n\nDirectory Structure\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)"
}
{
    "repo_name": "node",
    "watchers": "2.9k",
    "forks": "29.2k",
    "stars": "107k",
    "languages": {},
    "about_info": "Node.js JavaScript runtime ✨🐢🚀✨",
    "repo_url": "https://github.com/nodejs/node",
    "readme_content": "Node.js\nNode.js is an open-source, cross-platform JavaScript runtime environment.\nFor information on using Node.js, see the Node.js website.\nThe Node.js project uses an open governance model. The\nOpenJS Foundation provides support for the project.\nContributors are expected to act in a collaborative manner to move\nthe project forward. We encourage the constructive exchange of contrary\nopinions and compromise. The TSC\nreserves the right to limit or block contributors who repeatedly act in ways\nthat discourage, exhaust, or otherwise negatively affect other participants.\nThis project has a Code of Conduct.\nTable of contents\n\nSupport\nRelease types\n\nDownload\n\nCurrent and LTS releases\nNightly releases\nAPI documentation\n\n\nVerifying binaries\n\n\nBuilding Node.js\nSecurity\nContributing to Node.js\nCurrent project team members\n\nTSC (Technical Steering Committee)\nCollaborators\nTriagers\nRelease keys\n\n\nLicense\n\nSupport\nLooking for help? Check out the\ninstructions for getting support.\nRelease types\n\nCurrent: Under active development. Code for the Current release is in the\nbranch for its major version number (for example,\nv22.x). Node.js releases a new\nmajor version every 6 months, allowing for breaking changes. This happens in\nApril and October every year. Releases appearing each October have a support\nlife of 8 months. Releases appearing each April convert to LTS (see below)\neach October.\nLTS: Releases that receive Long Term Support, with a focus on stability\nand security. Every even-numbered major version will become an LTS release.\nLTS releases receive 12 months of Active LTS support and a further 18 months\nof Maintenance. LTS release lines have alphabetically-ordered code names,\nbeginning with v4 Argon. There are no breaking changes or feature additions,\nexcept in some special circumstances.\nNightly: Code from the Current branch built every 24-hours when there are\nchanges. Use with caution.\n\nCurrent and LTS releases follow semantic versioning. A\nmember of the Release Team signs each Current and LTS release.\nFor more information, see the\nRelease README.\nDownload\nBinaries, installers, and source tarballs are available at\nhttps://nodejs.org/en/download/.\nCurrent and LTS releases\nhttps://nodejs.org/download/release/\nThe latest directory is an\nalias for the latest Current release. The latest-codename directory is an\nalias for the latest release from an LTS line. For example, the\nlatest-hydrogen\ndirectory contains the latest Hydrogen (Node.js 18) release.\nNightly releases\nhttps://nodejs.org/download/nightly/\nEach directory and filename includes the version (e.g., v22.0.0),\nfollowed by the UTC date (e.g., 20240424 for April 24, 2024),\nand the short commit SHA of the HEAD of the release (e.g., ddd0a9e494).\nFor instance, a full directory name might look like v22.0.0-nightly20240424ddd0a9e494.\nAPI documentation\nDocumentation for the latest Current release is at https://nodejs.org/api/.\nVersion-specific documentation is available in each release directory in the\ndocs subdirectory. Version-specific documentation is also at\nhttps://nodejs.org/download/docs/.\nVerifying binaries\nDownload directories contain a SHASUMS256.txt file with SHA checksums for the\nfiles.\nTo download SHASUMS256.txt using curl:\ncurl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt\nTo check that a downloaded file matches the checksum, run\nit through sha256sum with a command such as:\ngrep node-vx.y.z.tar.gz SHASUMS256.txt | sha256sum -c -\nFor Current and LTS, the GPG detached signature of SHASUMS256.txt is in\nSHASUMS256.txt.sig. You can use it with gpg to verify the integrity of\nSHASUMS256.txt. You will first need to import\nthe GPG keys of individuals authorized to create releases.\nSee Release keys for commands to import active release keys.\nNext, download the SHASUMS256.txt.sig for the release:\ncurl -O https://nodejs.org/dist/vx.y.z/SHASUMS256.txt.sig\nThen use gpg --verify SHASUMS256.txt.sig SHASUMS256.txt to verify\nthe file's signature.\nBuilding Node.js\nSee BUILDING.md for instructions on how to build Node.js from\nsource and a list of supported platforms.\nSecurity\nFor information on reporting security vulnerabilities in Node.js, see\nSECURITY.md.\nContributing to Node.js\n\nContributing to the project\nWorking Groups\nStrategic initiatives\nTechnical values and prioritization\n\nCurrent project team members\nFor information about the governance of the Node.js project, see\nGOVERNANCE.md.\n\nTSC (Technical Steering Committee)\nTSC voting members\n\n\naduh95 -\nAntoine du Hamel <duhamelantoine1995@gmail.com> (he/him)\nanonrig -\nYagiz Nizipli <yagiz@nizipli.com> (he/him)\nbenjamingr -\nBenjamin Gruenbaum <benjamingr@gmail.com>\nBridgeAR -\nRuben Bridgewater <ruben@bridgewater.de> (he/him)\ngireeshpunathil -\nGireesh Punathil <gpunathi@in.ibm.com> (he/him)\njasnell -\nJames M Snell <jasnell@gmail.com> (he/him)\njoyeecheung -\nJoyee Cheung <joyeec9h3@gmail.com> (she/her)\nlegendecas -\nChengzhong Wu <legendecas@gmail.com> (he/him)\nmarco-ippolito -\nMarco Ippolito <marcoippolito54@gmail.com> (he/him)\nmcollina -\nMatteo Collina <matteo.collina@gmail.com> (he/him)\nmhdawson -\nMichael Dawson <midawson@redhat.com> (he/him)\nMoLow -\nMoshe Atlow <moshe@atlow.co.il> (he/him)\nRafaelGSS -\nRafael Gonzaga <rafael.nunu@hotmail.com> (he/him)\nrichardlau -\nRichard Lau <rlau@redhat.com>\nronag -\nRobert Nagy <ronagy@icloud.com>\nruyadorno -\nRuy Adorno <ruy@vlt.sh> (he/him)\nShogunPanda -\nPaolo Insogna <paolo@cowtech.it> (he/him)\ntargos -\nMichaël Zasso <targos@protonmail.com> (he/him)\ntniessen -\nTobias Nießen <tniessen@tnie.de> (he/him)\n\nTSC regular members\n\napapirovski -\nAnatoli Papirovski <apapirovski@mac.com> (he/him)\nBethGriggs -\nBeth Griggs <bethanyngriggs@gmail.com> (she/her)\nbnoordhuis -\nBen Noordhuis <info@bnoordhuis.nl>\ncjihrig -\nColin Ihrig <cjihrig@gmail.com> (he/him)\ncodebytere -\nShelley Vohr <shelley.vohr@gmail.com> (she/her)\nGeoffreyBooth -\nGeoffrey Booth <webadmin@geoffreybooth.com> (he/him)\nTrott -\nRich Trott <rtrott@gmail.com> (he/him)\n\n\nTSC emeriti members\nTSC emeriti members\n\naddaleax -\nAnna Henningsen <anna@addaleax.net> (she/her)\nChALkeR -\nСковорода Никита Андреевич <chalkerx@gmail.com> (he/him)\nchrisdickinson -\nChris Dickinson <christopher.s.dickinson@gmail.com>\ndanbev -\nDaniel Bevenius <daniel.bevenius@gmail.com> (he/him)\ndanielleadams -\nDanielle Adams <adamzdanielle@gmail.com> (she/her)\nevanlucas -\nEvan Lucas <evanlucas@me.com> (he/him)\nfhinkel -\nFranziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)\nFishrock123 -\nJeremiah Senkpiel <fishrock123@rocketmail.com> (he/they)\ngabrielschulhof -\nGabriel Schulhof <gabrielschulhof@gmail.com>\ngibfahn -\nGibson Fahnestock <gibfahn@gmail.com> (he/him)\nindutny -\nFedor Indutny <fedor@indutny.com>\nisaacs -\nIsaac Z. Schlueter <i@izs.me>\njoshgav -\nJosh Gavant <josh.gavant@outlook.com>\nmmarchini -\nMary Marchini <oss@mmarchini.me> (she/her)\nmscdex -\nBrian White <mscdex@mscdex.net>\nMylesBorins -\nMyles Borins <myles.borins@gmail.com> (he/him)\nnebrius -\nBryan Hughes <bryan@nebri.us>\nofrobots -\nAli Ijaz Sheikh <ofrobots@google.com> (he/him)\norangemocha -\nAlexis Campailla <orangemocha@nodejs.org>\npiscisaureus -\nBert Belder <bertbelder@gmail.com>\nRaisinTen -\nDarshan Sen <raisinten@gmail.com> (he/him)\nrvagg -\nRod Vagg <r@va.gg>\nsam-github -\nSam Roberts <vieuxtech@gmail.com>\nshigeki -\nShigeki Ohtsu <ohtsu@ohtsu.org> (he/him)\nthefourtheye -\nSakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)\nTimothyGu -\nTiancheng \"Timothy\" Gu <timothygu99@gmail.com> (he/him)\ntrevnorris -\nTrevor Norris <trev.norris@gmail.com>\n\n\n\nCollaborators\n\nabmusse -\nAbdirahim Musse <abdirahim.musse@ibm.com>\naddaleax -\nAnna Henningsen <anna@addaleax.net> (she/her)\naduh95 -\nAntoine du Hamel <duhamelantoine1995@gmail.com> (he/him) - Support me\nanonrig -\nYagiz Nizipli <yagiz@nizipli.com> (he/him) - Support me\napapirovski -\nAnatoli Papirovski <apapirovski@mac.com> (he/him)\natlowChemi -\nChemi Atlow <chemi@atlow.co.il> (he/him)\nAyase-252 -\nQingyu Deng <i@ayase-lab.com>\nbengl -\nBryan English <bryan@bryanenglish.com> (he/him)\nbenjamingr -\nBenjamin Gruenbaum <benjamingr@gmail.com>\nBethGriggs -\nBeth Griggs <bethanyngriggs@gmail.com> (she/her)\nbnb -\nTierney Cyren <hello@bnb.im> (they/them)\nbnoordhuis -\nBen Noordhuis <info@bnoordhuis.nl>\nBridgeAR -\nRuben Bridgewater <ruben@bridgewater.de> (he/him)\ncclauss -\nChristian Clauss <cclauss@me.com> (he/him)\ncjihrig -\nColin Ihrig <cjihrig@gmail.com> (he/him)\ncodebytere -\nShelley Vohr <shelley.vohr@gmail.com> (she/her)\ncola119 -\nKohei Ueno <kohei.ueno119@gmail.com> (he/him)\ndaeyeon -\nDaeyeon Jeong <daeyeon.dev@gmail.com> (he/him)\ndanielleadams -\nDanielle Adams <adamzdanielle@gmail.com> (she/her)\ndebadree25 -\nDebadree Chatterjee <debadree333@gmail.com> (he/him)\ndeokjinkim -\nDeokjin Kim <deokjin81.kim@gmail.com> (he/him)\nedsadr -\nAdrian Estrada <edsadr@gmail.com> (he/him)\nErickWendel -\nErick Wendel <erick.workspace@gmail.com> (he/him)\nEthan-Arrowood -\nEthan Arrowood <ethan@arrowood.dev> (he/him)\nF3n67u -\nFeng Yu <F3n67u@outlook.com> (he/him)\nfhinkel -\nFranziska Hinkelmann <franziska.hinkelmann@gmail.com> (she/her)\nFlarna -\nGerhard Stöbich <deb2001-github@yahoo.de> (he/they)\ngabrielschulhof -\nGabriel Schulhof <gabrielschulhof@gmail.com>\ngengjiawen -\nJiawen Geng <technicalcute@gmail.com>\nGeoffreyBooth -\nGeoffrey Booth <webadmin@geoffreybooth.com> (he/him)\ngireeshpunathil -\nGireesh Punathil <gpunathi@in.ibm.com> (he/him)\nguybedford -\nGuy Bedford <guybedford@gmail.com> (he/him)\nH4ad -\nVinícius Lourenço Claro Cardoso <contact@viniciusl.com.br> (he/him)\nHarshithaKP -\nHarshitha K P <harshitha014@gmail.com> (she/her)\nhimself65 -\nZeyu \"Alex\" Yang <himself65@outlook.com> (he/him)\njakecastelli -\nJake Yuesong Li <jake.yuesong@gmail.com> (he/him)\nJakobJingleheimer -\nJacob Smith <jacob@frende.me> (he/him)\njasnell -\nJames M Snell <jasnell@gmail.com> (he/him)\njkrems -\nJan Krems <jan.krems@gmail.com> (he/him)\njoesepi -\nJoe Sepi <sepi@joesepi.com> (he/him)\njoyeecheung -\nJoyee Cheung <joyeec9h3@gmail.com> (she/her)\njuanarbol -\nJuan José Arboleda <soyjuanarbol@gmail.com> (he/him)\nJungMinu -\nMinwoo Jung <nodecorelab@gmail.com> (he/him)\nKhafraDev -\nMatthew Aitken <maitken033380023@gmail.com> (he/him)\nkvakil -\nKeyhan Vakil <kvakil@sylph.kvakil.me>\nlegendecas -\nChengzhong Wu <legendecas@gmail.com> (he/him)\nlemire -\nDaniel Lemire <daniel@lemire.me>\nLinkgoron -\nNitzan Uziely <linkgoron@gmail.com>\nLiviaMedeiros -\nLiviaMedeiros <livia@cirno.name>\nlpinca -\nLuigi Pinca <luigipinca@gmail.com> (he/him)\nlukekarrys -\nLuke Karrys <luke@lukekarrys.com> (he/him)\nLxxyx -\nZijian Liu <lxxyxzj@gmail.com> (he/him)\nmarco-ippolito -\nMarco Ippolito <marcoippolito54@gmail.com> (he/him) - Support me\nmarsonya -\nAkhil Marsonya <akhil.marsonya27@gmail.com> (he/him)\nMattiasBuelens -\nMattias Buelens <mattias@buelens.com> (he/him)\nmcollina -\nMatteo Collina <matteo.collina@gmail.com> (he/him) - Support me\nmeixg -\nXuguang Mei <meixuguang@gmail.com> (he/him)\nmhdawson -\nMichael Dawson <midawson@redhat.com> (he/him)\nmildsunrise -\nAlba Mendez <me@alba.sh> (she/her)\nMoLow -\nMoshe Atlow <moshe@atlow.co.il> (he/him)\nMrJithil -\nJithil P Ponnan <jithil@outlook.com> (he/him)\novflowd -\nClaudio Wunder <cwunder@gnome.org> (he/they)\npanva -\nFilip Skokan <panva.ip@gmail.com> (he/him)\npimterry -\nTim Perry <pimterry@gmail.com> (he/him)\nQard -\nStephen Belanger <admin@stephenbelanger.com> (he/him)\nRafaelGSS -\nRafael Gonzaga <rafael.nunu@hotmail.com> (he/him)\nrichardlau -\nRichard Lau <rlau@redhat.com>\nrluvaton -\nRaz Luvaton <rluvaton@gmail.com> (he/him)\nronag -\nRobert Nagy <ronagy@icloud.com>\nruyadorno -\nRuy Adorno <ruy@vlt.sh> (he/him)\nsantigimeno -\nSantiago Gimeno <santiago.gimeno@gmail.com>\nShogunPanda -\nPaolo Insogna <paolo@cowtech.it> (he/him)\nsrl295 -\nSteven R Loomis <srl295@gmail.com>\nStefanStojanovic -\nStefan Stojanovic <stefan.stojanovic@janeasystems.com> (he/him)\nsxa -\nStewart X Addison <sxa@redhat.com> (he/him)\ntargos -\nMichaël Zasso <targos@protonmail.com> (he/him)\ntheanarkh -\ntheanarkh <theratliter@gmail.com> (he/him)\ntniessen -\nTobias Nießen <tniessen@tnie.de> (he/him)\ntrivikr -\nTrivikram Kamat <trivikr.dev@gmail.com>\nTrott -\nRich Trott <rtrott@gmail.com> (he/him)\nUlisesGascon -\nUlises Gascón <ulisesgascongonzalez@gmail.com> (he/him)\nvmoroz -\nVladimir Morozov <vmorozov@microsoft.com> (he/him)\nVoltrexKeyva -\nMohammed Keyvanzadeh <mohammadkeyvanzade94@gmail.com> (he/him)\nwatilde -\nDaijiro Wachi <daijiro.wachi@gmail.com> (he/him)\nzcbenz -\nCheng Zhao <zcbenz@gmail.com> (he/him)\nZYSzys -\nYongsheng Zhang <zyszys98@gmail.com> (he/him)\n\n\nEmeriti\n\nCollaborator emeriti\n\nak239 -\nAleksei Koziatinskii <ak239spb@gmail.com>\nandrasq -\nAndras <andras@kinvey.com>\nAndreasMadsen -\nAndreas Madsen <amwebdk@gmail.com> (he/him)\nAnnaMag -\nAnna M. Kedzierska <anna.m.kedzierska@gmail.com>\nantsmartian -\nAnto Aravinth <anto.aravinth.cse@gmail.com> (he/him)\naqrln -\nAlexey Orlenko <eaglexrlnk@gmail.com> (he/him)\nAshCripps -\nAsh Cripps <email@ashleycripps.co.uk>\nbcoe -\nBen Coe <bencoe@gmail.com> (he/him)\nbmeck -\nBradley Farias <bradley.meck@gmail.com>\nbmeurer -\nBenedikt Meurer <benedikt.meurer@gmail.com>\nboneskull -\nChristopher Hiller <boneskull@boneskull.com> (he/him)\nbrendanashworth -\nBrendan Ashworth <brendan.ashworth@me.com>\nbzoz -\nBartosz Sosnowski <bartosz@janeasystems.com>\ncalvinmetcalf -\nCalvin Metcalf <calvin.metcalf@gmail.com>\nChALkeR -\nСковорода Никита Андреевич <chalkerx@gmail.com> (he/him)\nchrisdickinson -\nChris Dickinson <christopher.s.dickinson@gmail.com>\nclaudiorodriguez -\nClaudio Rodriguez <cjrodr@yahoo.com>\ndanbev -\nDaniel Bevenius <daniel.bevenius@gmail.com> (he/him)\nDavidCai1993 -\nDavid Cai <davidcai1993@yahoo.com> (he/him)\ndavisjam -\nJamie Davis <davisjam@vt.edu> (he/him)\ndevnexen -\nDavid Carlier <devnexen@gmail.com>\ndevsnek -\nGus Caplan <me@gus.host> (they/them)\ndigitalinfinity -\nHitesh Kanwathirtha <digitalinfinity@gmail.com> (he/him)\ndmabupt -\nXu Meng <dmabupt@gmail.com> (he/him)\ndnlup\ndnlup <dnlup.dev@gmail.com>\neljefedelrodeodeljefe -\nRobert Jefe Lindstaedt <robert.lindstaedt@gmail.com>\nestliberitas -\nAlexander Makarenko <estliberitas@gmail.com>\neugeneo -\nEugene Ostroukhov <eostroukhov@google.com>\nevanlucas -\nEvan Lucas <evanlucas@me.com> (he/him)\nfiredfox -\nDaniel Wang <wangyang0123@gmail.com>\nFishrock123 -\nJeremiah Senkpiel <fishrock123@rocketmail.com> (he/they)\ngdams -\nGeorge Adams <gadams@microsoft.com> (he/him)\ngeek -\nWyatt Preul <wpreul@gmail.com>\ngibfahn -\nGibson Fahnestock <gibfahn@gmail.com> (he/him)\nglentiki -\nGlen Keane <glenkeane.94@gmail.com> (he/him)\nhashseed -\nYang Guo <yangguo@chromium.org> (he/him)\nhiroppy -\nYuta Hiroto <hello@hiroppy.me> (he/him)\niansu -\nIan Sutherland <ian@iansutherland.ca>\niarna -\nRebecca Turner <me@re-becca.org>\nimran-iq -\nImran Iqbal <imran@imraniqbal.org>\nimyller -\nIlkka Myller <ilkka.myller@nodefield.com>\nindutny -\nFedor Indutny <fedor@indutny.com>\nisaacs -\nIsaac Z. Schlueter <i@izs.me>\nitaloacasas -\nItalo A. Casas <me@italoacasas.com> (he/him)\nJacksonTian -\nJackson Tian <shyvo1987@gmail.com>\njasongin -\nJason Ginchereau <jasongin@microsoft.com>\njbergstroem -\nJohan Bergström <bugs@bergstroem.nu>\njdalton -\nJohn-David Dalton <john.david.dalton@gmail.com>\njhamhader -\nYuval Brik <yuval@brik.org.il>\njoaocgreis -\nJoão Reis <reis@janeasystems.com>\njoshgav -\nJosh Gavant <josh.gavant@outlook.com>\njulianduque -\nJulian Duque <julianduquej@gmail.com> (he/him)\nkfarnung -\nKyle Farnung <kfarnung@microsoft.com> (he/him)\nkunalspathak -\nKunal Pathak <kunal.pathak@microsoft.com>\nkuriyosh -\nYoshiki Kurihara <yosyos0306@gmail.com> (he/him)\nlance -\nLance Ball <lball@redhat.com> (he/him)\nLeko -\nShingo Inoue <leko.noor@gmail.com> (he/him)\nlucamaraschi -\nLuca Maraschi <luca.maraschi@gmail.com> (he/him)\nlundibundi -\nDenys Otrishko <shishugi@gmail.com> (he/him)\nlxe -\nAleksey Smolenchuk <lxe@lxe.co>\nmaclover7 -\nJon Moss <me@jonathanmoss.me> (he/him)\nmafintosh -\nMathias Buus <mathiasbuus@gmail.com> (he/him)\nmatthewloring -\nMatthew Loring <mattloring@google.com>\nMesteery -\nMestery <mestery@protonmail.com> (he/him)\nmicnic -\nNicu Micleușanu <micnic90@gmail.com> (he/him)\nmikeal -\nMikeal Rogers <mikeal.rogers@gmail.com>\nmiladfarca -\nMilad Fa <mfarazma@redhat.com> (he/him)\nmisterdjules -\nJulien Gilli <jgilli@netflix.com>\nmmarchini -\nMary Marchini <oss@mmarchini.me> (she/her)\nmonsanto -\nChristopher Monsanto <chris@monsan.to>\nMoonBall -\nChen Gang <gangc.cxy@foxmail.com>\nmscdex -\nBrian White <mscdex@mscdex.net>\nMylesBorins -\nMyles Borins <myles.borins@gmail.com> (he/him)\nnot-an-aardvark -\nTeddy Katz <teddy.katz@gmail.com> (he/him)\nofrobots -\nAli Ijaz Sheikh <ofrobots@google.com> (he/him)\nOlegas -\nOleg Elifantiev <oleg@elifantiev.ru>\norangemocha -\nAlexis Campailla <orangemocha@nodejs.org>\nothiym23 -\nForrest L Norvell <ogd@aoaioxxysz.net> (they/them/themself)\noyyd -\nOuyang Yadong <oyydoibh@gmail.com> (he/him)\npetkaantonov -\nPetka Antonov <petka_antonov@hotmail.com>\nphillipj -\nPhillip Johnsen <johphi@gmail.com>\npiscisaureus -\nBert Belder <bertbelder@gmail.com>\npmq20 -\nMinqi Pan <pmq2001@gmail.com>\nPoojaDurgad -\nPooja D P <Pooja.D.P@ibm.com> (she/her)\nprincejwesley -\nPrince John Wesley <princejohnwesley@gmail.com>\npsmarshall -\nPeter Marshall <petermarshall@chromium.org> (he/him)\npuzpuzpuz -\nAndrey Pechkurov <apechkurov@gmail.com> (he/him)\nRaisinTen -\nDarshan Sen <raisinten@gmail.com> (he/him)\nrefack -\nRefael Ackermann (רפאל פלחי) <refack@gmail.com> (he/him/הוא/אתה)\nrexagod -\nPranshu Srivastava <rexagod@gmail.com> (he/him)\nrickyes -\nRicky Zhou <0x19951125@gmail.com> (he/him)\nrlidwka -\nAlex Kocharin <alex@kocharin.ru>\nrmg -\nRyan Graham <r.m.graham@gmail.com>\nrobertkowalski -\nRobert Kowalski <rok@kowalski.gd>\nromankl -\nRoman Klauke <romaaan.git@gmail.com>\nronkorving -\nRon Korving <ron@ronkorving.nl>\nRReverser -\nIngvar Stepanyan <me@rreverser.com>\nrubys -\nSam Ruby <rubys@intertwingly.net>\nrvagg -\nRod Vagg <rod@vagg.org>\nryzokuken -\nUjjwal Sharma <ryzokuken@disroot.org> (he/him)\nsaghul -\nSaúl Ibarra Corretgé <s@saghul.net>\nsam-github -\nSam Roberts <vieuxtech@gmail.com>\nsebdeckers -\nSebastiaan Deckers <sebdeckers83@gmail.com>\nseishun -\nNikolai Vavilov <vvnicholas@gmail.com>\nshigeki -\nShigeki Ohtsu <ohtsu@ohtsu.org> (he/him)\nshisama -\nMasashi Hirano <shisama07@gmail.com> (he/him)\nsilverwind -\nRoman Reiss <me@silverwind.io>\nstarkwang -\nWeijia Wang <starkwang@126.com>\nstefanmb -\nStefan Budeanu <stefan@budeanu.com>\ntellnes -\nChristian Tellnes <christian@tellnes.no>\nthefourtheye -\nSakthipriyan Vairamani <thechargingvolcano@gmail.com> (he/him)\nthlorenz -\nThorsten Lorenz <thlorenz@gmx.de>\nTimothyGu -\nTiancheng \"Timothy\" Gu <timothygu99@gmail.com> (he/him)\ntrevnorris -\nTrevor Norris <trev.norris@gmail.com>\ntunniclm -\nMike Tunnicliffe <m.j.tunnicliffe@gmail.com>\nvdeturckheim -\nVladimir de Turckheim <vlad2t@hotmail.com> (he/him)\nvkurchatkin -\nVladimir Kurchatkin <vladimir.kurchatkin@gmail.com>\nvsemozhetbyt -\nVse Mozhet Byt <vsemozhetbyt@gmail.com> (he/him)\nwatson -\nThomas Watson <w@tson.dk>\nwhitlockjc -\nJeremy Whitlock <jwhitlock@apache.org>\nXadillaX -\nKhaidi Chu <i@2333.moe> (he/him)\nyashLadha -\nYash Ladha <yash@yashladha.in> (he/him)\nyhwang -\nYihong Wang <yh.wang@ibm.com>\nyorkie -\nYorkie Liu <yorkiefixer@gmail.com>\nyosuke-furukawa -\nYosuke Furukawa <yosuke.furukawa@gmail.com>\n\n\n\nCollaborators follow the Collaborator Guide in\nmaintaining the Node.js project.\nTriagers\n\natlowChemi -\nChemi Atlow <chemi@atlow.co.il> (he/him)\nAyase-252 -\nQingyu Deng <i@ayase-lab.com>\nbmuenzenmeyer -\nBrian Muenzenmeyer <brian.muenzenmeyer@gmail.com> (he/him)\nCanadaHonk -\nOliver Medhurst <honk@goose.icu> (they/them)\ndaeyeon -\nDaeyeon Jeong <daeyeon.dev@gmail.com> (he/him)\nF3n67u -\nFeng Yu <F3n67u@outlook.com> (he/him)\ngireeshpunathil -\nGireesh Punathil <gpunathi@in.ibm.com> (he/him)\niam-frankqiu -\nFrank Qiu <iam.frankqiu@gmail.com> (he/him)\nKevinEady -\nKevin Eady <kevin.c.eady@gmail.com> (he/him)\nkvakil -\nKeyhan Vakil <kvakil@sylph.kvakil.me>\nmarsonya -\nAkhil Marsonya <akhil.marsonya27@gmail.com> (he/him)\nmeixg -\nXuguang Mei <meixuguang@gmail.com> (he/him)\nmertcanaltin -\nMert Can Altin <mertgold60@gmail.com>\npreveen-stack -\nPreveen Padmanabhan <wide4head@gmail.com> (he/him)\nRedYetiDev -\nAviv Keller <redyetidev@gmail.com> (they/them)\nVoltrexKeyva -\nMohammed Keyvanzadeh <mohammadkeyvanzade94@gmail.com> (he/him)\n\nTriagers follow the Triage Guide when\nresponding to new issues.\nRelease keys\nPrimary GPG keys for Node.js Releasers (some Releasers sign with subkeys):\n\nBeth Griggs <bethanyngriggs@gmail.com>\n4ED778F539E3634C779C87C6D7062848A1AB005C\nBryan English <bryan@bryanenglish.com>\n141F07595B7B3FFE74309A937405533BE57C7D57\nDanielle Adams <adamzdanielle@gmail.com>\n74F12602B6F1C4E913FAA37AD3A89613643B6201\nJuan José Arboleda <soyjuanarbol@gmail.com>\nDD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7\nMarco Ippolito <marcoippolito54@gmail.com>\nCC68F5A3106FF448322E48ED27F5E38D5B0A215F\nMichaël Zasso <targos@protonmail.com>\n8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600\nRafael Gonzaga <rafael.nunu@hotmail.com>\n890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4\nRichard Lau <rlau@redhat.com>\nC82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C\nRuy Adorno <ruyadorno@hotmail.com>\n108F52B48DB57BB0CC439B2997B01419BD92F80A\nUlises Gascón <ulisesgascongonzalez@gmail.com>\nA363A499291CBBC940DD62E41F10027AF002F8B0\n\nTo import the full set of trusted release keys (including subkeys possibly used\nto sign releases):\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 4ED778F539E3634C779C87C6D7062848A1AB005C # Beth Griggs\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 141F07595B7B3FFE74309A937405533BE57C7D57 # Bryan English\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 74F12602B6F1C4E913FAA37AD3A89613643B6201 # Danielle Adams\ngpg --keyserver hkps://keys.openpgp.org --recv-keys DD792F5973C6DE52C432CBDAC77ABFA00DDBF2B7 # Juan José Arboleda\ngpg --keyserver hkps://keys.openpgp.org --recv-keys CC68F5A3106FF448322E48ED27F5E38D5B0A215F # Marco Ippolito\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 8FCCA13FEF1D0C2E91008E09770F7A9A5AE15600 # Michaël Zasso\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 890C08DB8579162FEE0DF9DB8BEAB4DFCF555EF4 # Rafael Gonzaga\ngpg --keyserver hkps://keys.openpgp.org --recv-keys C82FA3AE1CBEDC6BE46B9360C43CEC45C17AB93C # Richard Lau\ngpg --keyserver hkps://keys.openpgp.org --recv-keys 108F52B48DB57BB0CC439B2997B01419BD92F80A # Ruy Adorno\ngpg --keyserver hkps://keys.openpgp.org --recv-keys A363A499291CBBC940DD62E41F10027AF002F8B0 # Ulises Gascón\nSee Verifying binaries for how to use these keys to\nverify a downloaded file.\n\nOther keys used to sign some previous releases\n\nChris Dickinson <christopher.s.dickinson@gmail.com>\n9554F04D7259F04124DE6B476D5A82AC7E37093B\nColin Ihrig <cjihrig@gmail.com>\n94AE36675C464D64BAFA68DD7434390BDBE9B9C5\nDanielle Adams <adamzdanielle@gmail.com>\n1C050899334244A8AF75E53792EF661D867B9DFA\nEvan Lucas <evanlucas@me.com>\nB9AE9905FFD7803F25714661B63B535A4C206CA9\nGibson Fahnestock <gibfahn@gmail.com>\n77984A986EBC2AA786BC0F66B01FBB92821C587A\nIsaac Z. Schlueter <i@izs.me>\n93C7E9E91B49E432C2F75674B0A78B0A6C481CF6\nItalo A. Casas <me@italoacasas.com>\n56730D5401028683275BD23C23EFEFE93C4CFFFE\nJames M Snell <jasnell@keybase.io>\n71DCFD284A79C3B38668286BC97EC7A07EDE3FC1\nJeremiah Senkpiel <fishrock@keybase.io>\nFD3A5288F042B6850C66B31F09FE44734EB7990E\nJuan José Arboleda <soyjuanarbol@gmail.com>\n61FC681DFB92A079F1685E77973F295594EC4689\nJulien Gilli <jgilli@fastmail.fm>\n114F43EE0176B71C7BC219DD50A3051F888C628D\nMyles Borins <myles.borins@gmail.com>\nC4F0DFFF4E8C1A8236409D08E73BC641CC11F4C8\nRod Vagg <rod@vagg.org>\nDD8F2338BAE7501E3DD5AC78C273792F7D83545D\nRuben Bridgewater <ruben@bridgewater.de>\nA48C2BEE680E841632CD4E44F07496B3EB3C1762\nShelley Vohr <shelley.vohr@gmail.com>\nB9E2F5981AA6E0CD28160D9FF13993A75599653C\nTimothy J Fontaine <tjfontaine@gmail.com>\n7937DFD2AB06298B2293C3187D33FF9D0246406D\n\n\nSecurity release stewards\nWhen possible, the commitment to take slots in the\nsecurity release steward rotation is made by companies in order\nto ensure individuals who act as security stewards have the\nsupport and recognition from their employer to be able to\nprioritize security releases. Security release stewards manage security\nreleases on a rotation basis as outlined in the\nsecurity release process.\n\nDatadog\n\nbengl -\nBryan English <bryan@bryanenglish.com> (he/him)\n\n\nNodeSource\n\njuanarbol -\nJuan José Arboleda <soyjuanarbol@gmail.com> (he/him)\nRafaelGSS -\nRafael Gonzaga <rafael.nunu@hotmail.com> (he/him)\n\n\nPlatformatic\n\nmcollina -\nMatteo Collina <matteo.collina@gmail.com> (he/him)\n\n\nRed Hat / IBM\n\njoesepi -\nJoe Sepi <joesepi@ibm.com> (he/him)\nmhdawson -\nMichael Dawson <midawson@redhat.com> (he/him)\n\n\n\nLicense\nNode.js is available under the\nMIT license. Node.js also includes\nexternal libraries that are available under a variety of licenses.  See\nLICENSE for the full\nlicense text."
}
{
    "repo_name": "netdata",
    "watchers": "1.4k",
    "forks": "5.9k",
    "stars": "70.7k",
    "languages": {},
    "about_info": "Architected for speed. Automated for easy. Monitoring and troubleshooting, transformed!",
    "repo_url": "https://github.com/netdata/netdata",
    "readme_content": "Monitor your servers, containers, and applications,in high-resolution and in real-time.\n\n\n  \n  \n  \n  \n  \n  \n  \n  \n\nVisit the Project's Home Page\n\nMENU: GETTING STARTED | HOW IT WORKS | FAQ | DOCS | COMMUNITY | CONTRIBUTE | LICENSE\n\nImportant 💡\nPeople get addicted to Netdata. Once you use it on your systems, there's no going back!\n\nNetdata is a high-performance, cloud-native, and on-premises observability platform designed to monitor metrics and logs with unparalleled efficiency. It delivers a simpler, faster, and significantly easier approach to real-time, low-latency monitoring for systems, containers, and applications. Netdata requires zero-configuration to get started, offering a powerful and comprehensive monitoring experience, out of the box.\nNetdata is also known for its cost-efficient, distributed design. Unlike traditional monitoring solutions that centralize data, Netdata distributes the code. Instead of funneling all data into a few central databases, Netdata processes data at the edge, keeping it close to the source. The smart open-source Netdata Agent acts as a distributed database, enabling the construction of complex observability pipelines with modular, Lego-like simplicity.\nNetdata provides A.I. insights for all monitored data, training machine learning models directly at the edge. This allows for fully automated and unsupervised anomaly detection, and with its intuitive APIs and UIs, users can quickly perform root cause analysis and troubleshoot issues, identifying correlations and gaining deeper insights into their infrastructure.\nThe Netdata Ecosystem\nNetdata is built on three core components:\n\n\nNetdata Agent (usually called just \"Netdata\"): This open-source component is the heart of the Netdata ecosystem, handling data collection, storage (embedded database), querying, machine learning, exporting, and alerting of observability data. All observability data and features a Netdata ecosystem offers, are managed by the Netdata Agent. It runs in physical and virtual servers, cloud environments, Kubernetes clusters, and edge/IoT devices and is carefully optimized to have zero impact on production systems and applications.\n  \n\n\nNetdata Cloud: Enhancing the Netdata Agent, Netdata Cloud offers enterprise features such as user management, role-based access control, horizontal scalability, alert and notification management, access from anywhere, and more.  Netdata Cloud does not centralize or store observability data.\nNetdata Cloud is a commercial product, available as an on-premises installation, or a SaaS solution, with a free community tier.\n\n\nNetdata UI: The user interface that powers all dashboards, data visualization, and configuration.\nWhile closed-source, it is free to use with both Netdata Agents and Netdata Cloud, via their public APIs. It is included in the binary packages offered by Netdata and its latest version is publicly available via a CDN.\n\n\n\nNetdata scales effortlessly from a single server to thousands, even in complex, multi-cloud or hybrid environments, with the ability to retain data for years.\nKey characteristics of the Netdata Agent\n\n\n💥 Collects data from 800+ integrations\nOperating system metrics, container metrics, virtual machines, hardware sensors, applications metrics, OpenMetrics exporters, StatsD, and logs. OpenTelemetry is on its way to be included (currently being developed)...\n\n\n💪 Real-Time, Low-Latency, High-Resolution\nAll data are collected per second and are made available on the APIs for visualization, immediately after data collection (1-second latency, data collection to visualization).\n\n\n😶‍🌫️ AI across the board\nTrains multiple Machine-Learning (ML) models at the edge, for each metric collected and uses AI to detect anomalies based on the past behavior of each metric.\n\n\n📜  systemd-journald Logs\nIncludes tools to efficiently convert plain text log (text, csv, logfmt, json) files to structured systemd-journald entries (log2journal, systemd-cat-native) and queries systemd-journal files directly enabling powerful logs visualization dashboards. The Netdata Agents eliminate the need to centralize logs and provide all the functions to work with logs directly at the edge.\n\n\n⭐ Lego like, Observability Pipelines\nNetdata Agents can be linked to together (in parent-child relationships), to build observability centralization points within your infrastructure, allowing you to control data replication and retention at multiple levels.\n\n\n🔥 Fully Automated Powerful Visualization\nUsing the NIDL (Nodes, Instances, Dimensions & Labels) data model, the Netdata Agent enables the creation of fully automated dashboards, providing corellated visualization of all metrics, allowing you to understand any dataset at first sight, but also to filter, slice and dice the data directly on the dashboards, without the need to learn a query language.\nNote: the Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud.\n\n\n🔔 Out of box Alerts\nComes with hundreds of alerts out of the box to detect common issues and pitfalls, revealing issues that can easily go unnoticed. It supports several notification methods to let you know when your attention is needed.\n\n\n😎 Low Maintenance\nFully automated in every aspect: automated dashboards, out-of-the-box alerts, auto-detection and auto-discovery of metrics, zero-touch machine-learning, easy scalability and high availability, and CI/CD friendly.\n\n\n⭐ Open and Extensible\nNetdata is a modular platform that can be extended in all possible ways, and it also integrates nicely with other monitoring solutions.\n\n\nWhat can be monitored with the Netdata Agent\nNetdata monitors all the following:\n\n\n\nComponent\nLinux\nFreeBSD\nmacOS\nWindows*\n\n\n\n\nSystem ResourcesCPU, Memory and system shared resources\nFull\nYes\nYes\nYes\n\n\nStorageDisks, Mount points, Filesystems, RAID arrays\nFull\nBasic\nBasic\nBasic\n\n\nNetworkNetwork Interfaces, Protocols, Firewall, etc\nFull\nBasic\nBasic\nBasic\n\n\nHardware & SensorsFans, Temperatures, Controllers, GPUs, etc\nFull\nSome\nSome\nSome\n\n\nO/S ServicesResources, Performance and Status\nYessystemd\n-\n-\nBasic\n\n\nLogs\nYessystemd-journal\n-\n-\n-\n\n\nProcessesResources, Performance, OOM, and more\nYes\nYes\nYes\nYes\n\n\nNetwork ConnectionsLive TCP and UDP sockets per PID\nYes\n-\n-\n-\n\n\nContainersDocker/containerd, LXC/LXD, Kubernetes, etc\nYes\n-\n-\n-\n\n\nVMs (from the host)KVM, qemu, libvirt, Proxmox, etc\nYescgroups\n-\n-\nYesHyper-V\n\n\nSynthetic ChecksTest APIs, TCP ports, Ping, Certificates, etc\nYes\nYes\nYes\nYes\n\n\nPackaged Applicationsnginx, apache, postgres, redis, mongodb,and hundreds more\nYes\nYes\nYes\nYes\n\n\nCloud Provider InfrastructureAWS, GCP, Azure, and more\nYes\nYes\nYes\nYes\n\n\nCustom ApplicationsOpenMetrics, StatsD and soon OpenTelemetry\nYes\nYes\nYes\nYes\n\n\n\nWhen the Netdata Agent runs on Linux, it monitors every kernel feature available, providing full coverage of all kernel technologies that can be monitored.\nThe Netdata Agent also provides full enterprise hardware coverage, monitoring all components that provide hardware error reporting, like PCI AER, RAM EDAC, IPMI, S.M.A.R.T., NVMe, Fans, Power, Voltages, and more.\n * The Netdata Agent runs on Linux, FreeBSD and macOS. For Windows, we currently rely on Windows Exporter (so a Netdata running on Linux, FreeBSD or macOS is required, next to the monitored Windows servers). However, a Windows version of the Netdata Agent is at its final state for release.\n\n⭐ Netdata is the most energy-efficient monitoring tool ⭐\n\n\n  \n\n\n  \n\n\nDec 11, 2023: University of Amsterdam published a study related to the impact of monitoring tools for Docker based systems, aiming to answer 2 questions:\n\nThe impact of monitoring on the energy efficiency of Docker-based systems\nThe impact of monitoring on Docker-based systems?\n\n\n🚀 Netdata excels in energy efficiency: \"... Netdata being the most energy-efficient tool ...\", as the study says.\n🚀 Netdata excels in CPU Usage, RAM Usage and Execution Time, and has a similar impact in Network Traffic as Prometheus.\n\nThe study did not normalize the results based on the number of metrics collected. Given that Netdata usually collects significantly more metrics than the other tools, Netdata managed to outperform the other tools, while ingesting a much higher number of metrics. Read the full study here.\n\nNetdata vs Prometheus\n\n\n  \n\n\n  \n\n\nOn the same workload, Netdata uses 35% less CPU, 49% less RAM, 12% less bandwidth, 98% less disk I/O, and is 75% more disk space efficient on high resolution metrics storage, while providing more than a year of overall retention on the same disk footprint Prometheus offers 7 days of retention. Read the full analysis in our blog.\n\n \n\n  \n  \n  \n  Netdata actively supports and is a member of the Cloud Native Computing Foundation (CNCF)\n   \n  ...and due to your love ❤️, it is one of the most ⭐'d projects in the CNCF landscape!\n\n \n\n\n  Below is an animated image, but you can see Netdata live!\n\tFRANKFURT |\n\tNEWYORK |\n\tATLANTA |\n\tSANFRANCISCO |\n\tTORONTO |\n\tSINGAPORE |\n\tBANGALORE\n  \n  \tThey are clustered Netdata Agent Parents. They all have the same data. Select the one closer to you.\n\t\n\tAll these run with the default configuration. We only clustered them to have multi-node dashboards.\n\tNote: These demos include the Netdata UI,which while being closed-source, is free to use with Netdata Agents and Netdata Cloud.\n\n\n\nGetting Started\n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n1. Install Netdata everywhere ✌️\nNetdata can be installed on all Linux, macOS, FreeBSD (and soon on Windows) systems. We provide binary packages for the most popular operating systems and package managers.\n\nInstall on Ubuntu, Debian CentOS, Fedora, Suse, Red Hat, Arch, Alpine, Gentoo, even BusyBox.\nInstall with Docker.\nNetdata is a Verified Publisher on DockerHub and our users enjoy free unlimited DockerHub pulls 😍.\nInstall on macOS 🤘.\nInstall on FreeBSD and pfSense.\nInstall from source \nFor Kubernetes deployments check here.\n\nCheck also the Netdata Deployment Guides to decide how to deploy it in your infrastructure.\nBy default, you will have immediately available a local dashboard. Netdata starts a web server for its dashboard at port 19999. Open up your web browser of choice and\nnavigate to http://NODE:19999, replacing NODE with the IP address or hostname of your Agent. If installed on localhost, you can access it through http://localhost:19999.\nNote: the binary packages we provide, install Netdata UI automatically. Netdata UI is closed-source, but free to use with Netdata Agents and Netdata Cloud.\n2. Configure Collectors 💥\nNetdata auto-detects and auto-discovers most operating system data sources and applications. However, many data sources require some manual configuration, usually to allow Netdata to get access to the metrics.\n\nFor a detailed list of the 800+ collectors available, check this guide.\nTo monitor Windows servers and applications use this guide.Note that Netdata on Windows is at its final release stage, so at the next Netdata release Netdata will natively support Windows.\nTo monitor SNMP devices check this guide.\n\n3. Configure Alert Notifications 🔔\nNetdata comes with hundreds of pre-configured alerts, that automatically check your metrics, immediately after they start getting collected.\nNetdata can dispatch alert notifications to multiple third party systems, including: email, Alerta, AWS SNS, Discord, Dynatrace, flock, gotify, IRC, Matrix, MessageBird, Microsoft Teams, ntfy, OPSgenie, PagerDuty, Prowl, PushBullet, PushOver, RocketChat, Slack, SMS tools, Syslog, Telegram, Twilio.\nBy default, Netdata will send e-mail notifications, if there is a configured MTA on the system.\n4. Configure Netdata Parents 👪\nOptionally, configure one or more Netdata Parents. A Netdata Parent is a Netdata Agent that has been configured to accept streaming connections from other Netdata agents.\nNetdata Parents provide:\n\n\nInfrastructure level dashboards, at http://parent.server.ip:19999/.\nEach Netdata Agent has an API listening at the TCP port 19999 of each server.\nWhen you hit that port with a web browser (e.g. http://server.ip:19999/), the Netdata Agent UI is presented.\nWhen the Netdata Agent is also a Parent, the UI of the Parent includes data for all nodes that stream metrics to that Parent.\n\n\nIncreased retention for all metrics of all your nodes.\nEach Netdata Agent maintains each own database of metrics. But Parents can be given additional resources to maintain a much longer database than\nindividual Netdata Agents.\n\n\nCentral configuration of alerts and dispatch of notifications.\nUsing Netdata Parents, all the alert notifications integrations can be configured only once, at the Parent and they can be disabled at the Netdata Agents.\n\n\nYou can also use Netdata Parents to:\n\nOffload your production systems (the parents run ML, alerts, queries, etc. for all their children)\nSecure your production systems (the parents accept user connections, for all their children)\n\n5. Connect to Netdata Cloud ☁️\nSign-in to Netdata Cloud and claim your Netdata Agents and Parents.\nIf you connect your Netdata Parents, there is no need to connect your Netdata Agents. They will be connected via the Parents.\nWhen your Netdata nodes are connected to Netdata Cloud, you can (on top of the above):\n\nAccess your Netdata agents from anywhere\nAccess sensitive Netdata agent features (like \"Netdata Functions\": processes, systemd-journal)\nOrganize your infra in spaces and Rooms\nCreate, manage, and share custom dashboards\nInvite your team and assign roles to them (Role Based Access Control - RBAC)\nGet infinite horizontal scalability (multiple independent Netdata Agents are viewed as one infra)\nConfigure alerts from the UI\nConfigure data collection from the UI\nNetdata Mobile App notifications\n\n🤟 Netdata Cloud does not prevent you from using your Netdata Agents and Parents directly, and vice versa.\n👌 Your metrics are still stored in your network when you connect your Netdata Agents and Parents to Netdata Cloud.\n\nHow it works\nNetdata is built around a modular metrics processing pipeline.\nClick to see more details about this pipeline...\n \nEach Netdata Agent can perform the following functions:\n\n\nCOLLECT metrics from their sources\nUses internal and external plugins to collect data from their sources.\nNetdata auto-detects and collects almost everything from the operating system: including CPU, Interrupts, Memory, Disks, Mount Points, Filesystems, Network Stack, Network Interfaces, Containers, VMs, Processes, systemd units, Linux Performance Metrics, Linux eBPF, Hardware Sensors, IPMI, and more.\nIt collects application metrics from applications: PostgreSQL, MySQL/MariaDB, Redis, MongoDB, Nginx, Apache, and hundreds more.\nNetdata also collects your custom application metrics by scraping OpenMetrics exporters, or via StatsD.\nIt can convert web server log files to metrics and apply ML and alerts to them, in real-time.\nAnd it also supports synthetic tests / white box tests, so you can ping servers, check API responses, or even check filesystem files and directories to generate metrics, train ML and run alerts and notifications on their status.\n\n\nSTORE metrics to a database\nUses database engine plugins to store the collected data, either in memory and/or on disk. We have developed our own dbengine for storing the data in a very efficient manner, allowing Netdata to have less than 1 byte per sample on disk and amazingly fast queries.\n\n\nLEARN the behavior of metrics (ML)\nTrains multiple Machine-Learning (ML) models per metric to learn the behavior of each metric individually. Netdata uses the kmeans algorithm and creates by default a model per metric per hour, based on the values collected for that metric over the last 6 hours. The trained models are persisted to disk.\n\n\nDETECT anomalies in metrics (ML)\nUses the trained machine learning (ML) models to detect outliers and mark collected samples as anomalies. Netdata stores anomaly information together with each sample and also streams it to Netdata Parents so that the anomaly is also available at query time for the whole retention of each metric.\n\n\nCHECK metrics and trigger alert notifications\nUses its configured alerts (you can configure your own) to check the metrics for common issues and uses notifications plugins to send alert notifications.\n\n\nSTREAM metrics to other Netdata Agents\nPush metrics in real-time to Netdata Parents.\n\n\nARCHIVE metrics to 3rd party databases\nExport metrics to industry standard time-series databases, like Prometheus, InfluxDB, OpenTSDB, Graphite, etc.\n\n\nQUERY metrics and present dashboards\nProvide an API to query the data and present interactive dashboards to users.\n\n\nSCORE metrics to reveal similarities and patterns\nScore the metrics according to the given criteria, to find the needle in the haystack.\n\n\nWhen using Netdata Parents, all the functions of a Netdata Agent (except data collection) can be delegated to Parents to offload production systems.\nThe core of Netdata is developed in C. We have our own libnetdata, that provides:\n\n\nDICTIONARY\nA high-performance algorithm to maintain both indexed and ordered pools of structures Netdata needs. It uses JudyHS arrays for indexing, although it is modular: any hashtable or tree can be integrated into it. Despite being in C, dictionaries follow object-oriented programming principles, so there are constructors, destructors, automatic memory management, garbage collection, and more. For more see here.\n\n\nARAL\nARray ALlocator (ARAL) is used to minimize the system allocations made by Netdata. ARAL is optimized for maximum multithreaded performance. It also allows all structures that use it to be allocated in memory-mapped files (shared memory) instead of RAM. For more see here.\n\n\nPROCFILE\nA high-performance /proc (but also any) file parser and text tokenizer. It achieves its performance by keeping files open and adjusting its buffers to read the entire file in one call (which is also required by the Linux kernel). For more see here.\n\n\nSTRING\nA string internet mechanism, for string deduplication and indexing (using JudyHS arrays), optimized for multithreaded usage. For more see here.\n\n\nARL\nAdaptive Resortable List (ARL), is a very fast list iterator, that keeps the expected items on the list in the same order they are found in input list. So, the first iteration is somewhat slower, but all the following iterations are perfectly aligned for best performance. For more see here.\n\n\nBUFFER\nA flexible text buffer management system that allows Netdata to automatically handle dynamically sized text buffer allocations. The same mechanism is used for generating consistent JSON output by the Netdata APIs. For more see here.\n\n\nSPINLOCK\nLike POSIX MUTEX and RWLOCK but a lot faster, based on atomic operations, with significantly smaller memory impact, while being portable.\n\n\nPGC\nA caching layer that can be used to cache any kind of time-related data, with automatic indexing (based on a tree of JudyL arrays), memory management, evictions, flushing, pressure management. This is extensively used in dbengine. For more see here.\n\n\nThe above, and many more, allow Netdata developers to work on the application fast and with confidence. Most of the business logic in Netdata is a work of mixing the above.\nNetdata data collection plugins can be developed in any language. Most of our application collectors though are developed in Go.\n\nFAQ\n🛡️ Is Netdata secure?\nOf course, it is! We do our best to ensure it is!\nClick to see detailed answer ...\n  \nWe understand that Netdata is a software piece that is installed on millions of production systems across the world. So, it is important for us, Netdata to be as secure as possible:\n\nWe follow the Open Source Security Foundation best practices.\nWe have given great attention to detail when it comes to security design. Check out our security design.\nNetdata is a popular open-source project and is frequently tested by many security analysts.\nCheck also our security policies and advisories published so far.\n\n  \n\n🌀 Will Netdata consume significant resources on my servers?\nNo. It will not! We promise this will be fast!\nClick to see detailed answer ...\n  \nAlthough each Netdata Agent is a complete monitoring solution packed into a single application, and despite the fact that Netdata collects every metric every single second and trains multiple ML models per metric, you will find that Netdata has amazing performance! In many cases, it outperforms other monitoring solutions that have significantly fewer features or far smaller data collection rates.\nThis is what you should expect:\n\n\nFor production systems, each Netdata Agent with default settings (everything enabled, ML, Health, DB) should consume about 5% CPU utilization of one core and about 150 MiB or RAM.\nBy using a Netdata parent and streaming all metrics to that parent, you can disable ML & health and use an ephemeral DB (like alloc) on the children, leading to utilization of about 1% CPU of a single core and 100 MiB of RAM. Of course, these depend on how many metrics are collected.\n\n\nFor Netdata Parents, for about 1 to 2 million metrics, all collected every second, we suggest a server with 16 cores and 32GB RAM. Less than half of it will be used for data collection and ML. The rest will be available for queries.\n\n\nNetdata has extensive internal instrumentation to help us reveal how the resources consumed are used. All these are available in the \"Netdata Monitoring\" section of the dashboard. Depending on your use case, there are many options to optimize resource consumption.\nEven if you need to run Netdata on extremely weak embedded or IoT systems, you will find that Netdata can be tuned to be very performant.\n  \n\n📜 How much retention can I have?\nAs much as you need!\nClick to see detailed answer ...\n  \nNetdata supports tiering, to downsample past data and save disk space. With default settings, it has 3 tiers:\n\ntier 0, with high resolution, per-second, data.\ntier 1, mid-resolution, per minute, data.\ntier 2, low-resolution, per hour, data.\n\nAll tiers are updated in parallel during data collection. Just increase the disk space you give to Netdata to get a longer history for your metrics. Tiers are automatically chosen at query time depending on the time frame and the resolution requested.\n  \n\n🚀 Does it scale? I have really a lot of servers!\nNetdata is designed to scale and can handle large volumes of data.\nClick to see detailed answer ...\n  \nNetdata is a distributed monitoring solution. You can scale it to infinity by spreading Netdata Agents across your infrastructure.\nWith the streaming feature of the Agent, we can support monitoring ephemeral servers but also allow the creation of \"monitoring islands\" where metrics are aggregated to a few servers (Netdata Parents) for increased retention, or for offloading production systems.\n\n\n✈️ Netdata Parents provide great vertical scalability, so you can have as big parents as the CPU, RAM and Disk resources you can dedicate to them. In our lab we constantly stress test Netdata Parents with several million metrics collected per second, to ensure it is reliable, stable, and robust at scale.\n\n\n🚀 In addition, Netdata Cloud provides virtually unlimited horizontal scalability. It \"merges\" all the Netdata parents you have into one unified infrastructure at query time. Netdata Cloud itself is probably the biggest single installation monitoring platform ever created, currently monitoring about 100k online servers with about 10k servers changing state (added/removed) per day!\n\n\nExample: the following chart comes from a single Netdata Parent. As you can see on it, 244 nodes stream to it metrics of about 20k running containers. On this specific chart there are 3 dimensions per container, so a total of about 60k time-series queries are executed to present it.\n\n  \n\n💾 My production servers are very sensitive in disk I/O. Can I use Netdata?\nYes, you can!\nClick to see detailed answer ...\n  \nThe Netdata Agent has been designed to spread disk writes across time. Each metric is flushed to disk every 17 minutes (1000 seconds), but metrics are flushed evenly across time, at an almost constant rate. Also, metrics are packed into bigger blocks we call extents and are compressed with ZSTD before saving them, to minimize the number of I/O operations made.\nThe Netdata Agent also employs direct I/O for all its database operations. By managing its own caches, Netdata avoids overburdening system caches, facilitating a harmonious coexistence with other applications.\nSingle node Agents (not Parents), should have a constant write rate of about 50 KiB/s or less, with some spikes above that every minute (flushing of tier 1) and higher spikes every hour (flushing of tier 2).\nHealth Alerts and Machine-Learning run queries to evaluate their expressions and learn from the metrics' patterns. These are also spread over time, so there should be an almost constant read rate too.\nTo make Netdata not use the disks at all, we suggest the following:\n\nUse database mode alloc or ram to disable writing metric data to disk.\nConfigure streaming to push in real-time all metrics to a Netdata Parent. The Netdata Parent will maintain metrics on disk for this node.\nDisable ML and health on this node. The Netdata Parent will do them for this node.\nUse the Netdata Parent to access the dashboard.\n\nUsing the above, the Netdata Agent on your production system will not use a disk.\n  \n\n🤨 How is Netdata different from a Prometheus and Grafana setup?\nNetdata is a \"ready to use\" monitoring solution. Prometheus and Grafana are tools to build your own monitoring solution.\nNetdata is also a lot faster, requires significantly less resources and puts almost no stress on the server it runs. For a performance comparison check this blog.\nClick to see detailed answer ...\n  \nFirst, we have to say that Prometheus as a time-series database and Grafana as a visualizer are excellent tools for what they do.\nHowever, we believe that such a setup is missing a key element: A Prometheus and Grafana setup assumes that you know everything about the metrics you collect and you understand deeply how they are structured, they should be queried and visualized.\nIn reality, this setup has a lot of problems. The vast number of technologies, operating systems, and applications we use in our modern stacks, makes it impossible for any single person to know and understand everything about anything. We get testimonials regularly from Netdata users across the biggest enterprises, that Netdata manages to reveal issues, anomalies and problems they were not aware of and they didn't even have the means to find or troubleshoot.\nSo, the biggest difference of Netdata to Prometheus, and Grafana, is that we decided that the tool needs to have a much better understanding of the components, the applications, and the metrics it monitors.\n\n\nWhen compared to Prometheus, Netdata needs for each metric much more than just a name, some labels, and a value over time. A metric in Netdata is a structured entity that correlates with other metrics in a certain way and has specific attributes that depict how it should be organized, treated, queried, and visualized. We call this the NIDL (Nodes, Instances, Dimensions, Labels) framework.\nMaintaining such an index is a challenge: first, because the raw metrics collected do not provide this information, so we have to add it, and second because we need to maintain this index for the lifetime of each metric, which with our current database retention, it is usually more than a year.\nAt the same time, Netdata provides better retention than Prometheus due to database tiering, scales easier than Prometheus due to streaming, supports anomaly detection and it has a metrics scoring engine to find the needle in the haystack when needed.\n\n\nWhen compared to Grafana, Netdata is fully automated. Grafana has more customization capabilities than Netdata, but Netdata presents fully functional dashboards by itself and most importantly it gives you the means to understand, analyze, filter, slice and dice the data without the need for you to edit queries or be aware of any peculiarities the underlying metrics may have.\nFurthermore, to help you when you need to find the needle in the haystack, Netdata has advanced troubleshooting tools provided by the Netdata metrics scoring engine, that allows it to score metrics based on their anomaly rate, their differences or similarities for any given time frame.\n\n\nStill, if you are already familiar with Prometheus and Grafana, Netdata integrates nicely with them, and we have reports from users who use Netdata with Prometheus and Grafana in production.\n  \n\n🤨 How is Netdata different from DataDog, New Relic, Dynatrace, X SaaS Provider?\nWith Netdata your data are always on-prem and your metrics are always high-resolution.\nClick to see detailed answer ...\n  \nMost commercial monitoring providers face a significant challenge: they centralize all metrics to their infrastructure and this is, inevitably, expensive. It leads them to one or more of the following:\n\nbe unrealistically expensive\nlimit the number of metrics they collect\nlimit the resolution of the metrics they collect\n\nAs a result, they try to find a balance: collect the least possible data, but collect enough to have something useful out of it.\nWe, at Netdata, see monitoring in a completely different way: monitoring systems should be built bottom-up and be rich in insights, so we focus on each component individually to collect, store, check and visualize everything related to each of them, and we make sure that all components are monitored. Each metric is important.\nThis is why Netdata trains multiple machine-learning models per metric, based exclusively on their own past (no sampling of data, no sharing of trained models) to detect anomalies based on the specific use case and workload each component is used.\nThis is also why Netdata alerts are attached to components (instances) and are configured with dynamic thresholds and rolling windows, instead of static values.\nThe distributed nature of Netdata helps scale this approach: your data is spread inside your infrastructure, as close to the edge as possible. Netdata is not one data lane. Each Netdata Agent is a data lane and all of them together build a massive distributed metrics processing pipeline that ensures all your infrastructure components and applications are monitored and operating as they should.\n  \n\n🤨 How is Netdata different from Nagios, Icinga, Zabbix, etc.?\nNetdata offers real-time, comprehensive monitoring and the ability to monitor everything, without any custom configuration required.\nClick to see detailed answer ...\n  \nWhile Nagios, Icinga, Zabbix, and other similar tools are powerful and highly customizable, they can be complex to set up and manage. Their flexibility often comes at the cost of ease-of-use, especially for users who are not systems administrators or do not have extensive experience with these tools. Additionally, these tools generally require you to know what you want to monitor in advance and configure it explicitly.\nNetdata, on the other hand, takes a different approach. It provides a \"ready to use\" monitoring solution with a focus on simplicity and comprehensiveness. It automatically detects and starts monitoring many different system metrics and applications out-of-the-box, without any need for custom configuration.\nIn comparison to these traditional monitoring tools, Netdata:\n\n\nProvides real-time, high-resolution metrics, as opposed to the often minute-level granularity that tools like Nagios, Icinga, and Zabbix provide.\n\n\nAutomatically generates meaningful, organized, and interactive visualizations of the collected data. Unlike other tools, where you have to manually create and organize graphs and dashboards, Netdata takes care of this for you.\n\n\nApplies machine learning to each individual metric to detect anomalies, providing more insightful and relevant alerts than static thresholds.\n\n\nIs designed to be distributed, so your data is spread inside your infrastructure, as close to the edge as possible. This approach is more scalable and avoids the potential bottleneck of a single centralized server.\n\n\nHas a more modern and user-friendly interface, making it easy for anyone, not just experienced administrators, to understand the health and performance of their systems.\n\n\nEven if you're already using Nagios, Icinga, Zabbix, or similar tools, you can use Netdata alongside them to augment your existing monitoring capabilities with real-time insights and user-friendly dashboards.\n  \n\n😳 I feel overwhelmed by the amount of information in Netdata. What should I do?\nNetdata is designed to provide comprehensive insights, but we understand that the richness of information might sometimes feel overwhelming. Here are some tips on how to navigate and utilize Netdata effectively...\nClick to see detailed answer ...\n  \nNetdata is indeed a very comprehensive monitoring tool. It's designed to provide you with as much information as possible about your system and applications, so that you can understand and address any issues that arise. However, we understand that the sheer amount of data can sometimes be overwhelming.\nHere are some suggestions on how to manage and navigate this wealth of information:\n\n\nStart with the Metrics Dashboard\nNetdata's Metrics Dashboard provides a high-level summary of your system's status. We have added summary tiles on almost every section, you reveal the information that is more important. This is a great place to start, as it can help you identify any major issues or trends at a glance.\n\n\nUse the Search Feature\nIf you're looking for specific information, you can use the search feature to find the relevant metrics or charts. This can help you avoid scrolling through all the data.\n\n\nCustomize your Dashboards\nNetdata allows you to create custom dashboards, which can help you focus on the metrics that are most important to you. Sign-in to Netdata and there you can have your custom dashboards. (coming soon to the agent dashboard too)\n\n\nLeverage Netdata's Anomaly Detection\nNetdata uses machine learning to detect anomalies in your metrics. This can help you identify potential issues before they become major problems. We have added an AR button above the dashboard table of contents to reveal the anomaly rate per section so that you can easily spot what could need your attention.\n\n\nTake Advantage of Netdata's Documentation and Blogs\nNetdata has extensive documentation that can help you understand the different metrics and how to interpret them. You can also find tutorials, guides, and best practices there.\n\n\nRemember, it's not necessary to understand every single metric or chart right away. Netdata is a powerful tool, and it can take some time to fully explore and understand all of its features. Start with the basics and gradually delve into more complex metrics as you become more comfortable with the tool.\n  \n\n☁️ Do I have to subscribe to Netdata Cloud?\nNetdata Cloud delivers the full suite of features and functionality that Netdata offers, including a free community tier.\nWhile our default onboarding process encourages users to take advantage of Netdata Cloud, including a complimentary one-month trial of our full business product, it is not mandatory. Users have the option to bypass this process entirely and still utilize the Netdata Agents along with the Netdata UI, without the need to sign up for Netdata Cloud.\nClick to see detailed answer ...\n  \nThe Netdata Agent dashboard and the Netdata Cloud dashboard are the same. Still, Netdata Cloud provides additional features, that the Netdata Agent is not capable of. These include:\n\nAccess your infrastructure from anywhere.\nHave SSO to protect sensitive features.\nCustomizable (custom dashboards and other settings are persisted when you are signed in to Netdata Cloud)\nConfiguration of Alerts and Data Collection from the UI\nSecurity (role-based access control - RBAC).\nHorizontal Scalability (\"blend\" multiple independent parents in one uniform infrastructure)\nCentral Dispatch of Alert Notifications (even when multiple independent parents are involved)\nMobile App for Alert Notifications\n\nWe encourage you to support Netdata by buying a Netdata Cloud subscription. A successful Netdata is a Netdata that evolves and gets improved to provide a simpler, faster and easier monitoring for all of us.\nFor organizations that need a fully on-prem solution, we provide Netdata Cloud for on-prem installation. Contact us for more information.\n  \n\n🔎 What does the anonymous telemetry collected by Netdata entail?\nYour privacy is our utmost priority. As part of our commitment to improving Netdata, we rely on anonymous telemetry data from our users who choose to leave it enabled. This data greatly informs our decision-making processes and contributes to the future evolution of Netdata.\nShould you wish to disable telemetry, instructions for doing so are provided in our installation guides.\nClick to see detailed answer ...\n  \nNetdata is in a constant state of growth and evolution. The decisions that guide this development are ideally rooted in data. By analyzing anonymous telemetry data, we can answer questions such as: \"What features are being used frequently?\", \"How do we prioritize between potential new features?\" and \"What elements of Netdata are most important to our users?\"\nBy leaving anonymous telemetry enabled, users indirectly contribute to shaping Netdata's roadmap, providing invaluable information that helps us prioritize our efforts for the project and the community.\nWe are aware that for privacy or regulatory reasons, not all environments can allow telemetry. To cater to this, we have simplified the process of disabling telemetry:\n\nDuring installation, you can append --disable-telemetry to our kickstart.sh script, or\nCreate the file /etc/netdata/.opt-out-from-anonymous-statistics and then restart Netdata.\n\nThese steps will disable the anonymous telemetry for your Netdata installation.\nPlease note, even with telemetry disabled, Netdata still requires a Netdata Registry for alert notifications' Call To Action (CTA) functionality. When you click an alert notification, it redirects you to the Netdata Registry, which then directs your web browser to the specific Netdata Agent that issued the alert for further troubleshooting. The Netdata Registry learns the URLs of your agents when you visit their dashboards.\nAny Netdata Agent can act as a Netdata Registry. Simply designate one Netdata Agent as your registry, and our global Netdata Registry will no longer be in use. For further information on this, please refer to this guide.\n  \n\n😏 Who uses Netdata?\nNetdata is a widely adopted project...\nClick to see detailed answer ...\n  \nBrowse the Netdata stargazers on GitHub to discover users from renowned companies and enterprises, such as ABN AMRO Bank, AMD, Amazon, Baidu, Booking.com, Cisco, Delta, Facebook, Google, IBM, Intel, Logitech, Netflix, Nokia, Qualcomm, Realtek Semiconductor Corp, Redhat, Riot Games, SAP, Samsung, Unity, Valve, and many others.\nNetdata also enjoys significant usage in academia, with notable institutions including New York University, Columbia University, New Jersey University, Seoul National University, University College London, among several others.\nAnd, Netdata is also used by numerous governmental organizations worldwide.\nIn a nutshell, Netdata proves invaluable for:\n\n\nInfrastructure intensive organizations\nSuch as hosting/cloud providers and companies with hundreds or thousands of nodes, who require a high-resolution, real-time monitoring solution for a comprehensive view of all their components and applications.\n\n\nTechnology operators\nThose in need of a standardized, comprehensive solution for round-the-clock operations. Netdata not only facilitates operational automation and provides controlled access for their operations engineers, but also enhances skill development over time.\n\n\nTechnology startups\nWho seek a feature-rich monitoring solution from the get-go.\n\n\nFreelancers\nWho seek a simple, efficient and straightforward solution without sacrificing performance and outcomes.\n\n\nProfessional SysAdmins and DevOps\nWho appreciate the fine details and understand the value of holistic monitoring from the ground up.\n\n\nEveryone else\nAll of us, who are tired of the inefficiency in the monitoring industry and would love a refreshing change and a breath of fresh air. 🙂\n\n\n  \n\n🌐 Is Netdata open-source?\nThe Netdata Agent is open-source, but the overall Netdata ecosystem is a hybrid solution, combining open-source and closed-source components.\nClick to see detailed answer ...\n  \nOpen-source is about sharing intellectual property with the world, and at Netdata, we embrace this philosophy wholeheartedly.\nThe Netdata Agent, the core of our ecosystem and the engine behind all our observability features, is fully open-source. Licensed under GPLv3+, the Netdata Agent represents our commitment to open-sourcing innovation in a wide array of observability technologies, including data collection, database design, query engines, observability data modeling, machine learning and unsupervised anomaly detection, high-performance edge computing, real-time monitoring, and more.\nThe Netdata Agent is our gift to the world, ensuring that the cutting-edge advancements we've developed are freely accessible to everyone.\nHowever, as a privately funded company, we also need to monetize our open-source software to demonstrate product-market fit and sustain our growth.\nTraditionally, open-source projects have often used the open-core model, where a basic version of the software is open-source, and additional features are reserved for a commercial, closed-source version. This approach can limit access to advanced innovations, as most of these remain closed-source.\nAt Netdata, we take a slightly different path. We don't create a separate enterprise version of our product. Instead, all users—whether commercial or not—utilize the same Netdata Agent, ensuring that all our observability innovations are always open-source.\nTo experience the full capabilities of the Netdata ecosystem, users need to combine the open-source components with our closed-source offerings. The complete product still remains free to use.\nThe closed-source components include:\n\nNetdata UI: This is closed-source but free to use with the Netdata Agents and Netdata Cloud. It’s also publicly available via a CDN.\nNetdata Cloud: A commercial product available both as an on-premises installation and as a SaaS solution, with a free community tier.\n\nBy balancing open-source and closed-source components, we ensure that all users have access to our innovations while sustaining our ability to grow and innovate as a company.\n  \n\n💰 What is your monetization strategy?\nNetdata generates revenue through subscriptions to advanced features of Netdata Cloud and sales of on-premise and private versions of Netdata Cloud.\nClick to see detailed answer ...\n  \nNetdata generates revenue from these activities:\n\n\nNetdata Cloud Subscriptions\nDirect funding for our project's vision comes from users subscribing to Netdata Cloud's advanced features.\n\n\nNetdata Cloud On-Prem or Private\nPurchasing the on-premises or private versions of Netdata Cloud supports our financial growth.\n\n\nOur Open-Source Community and the free access to Netdata Cloud, contribute to Netdata in the following ways:\n\n\nNetdata Cloud Community Use\nThe free usage of Netdata Cloud demonstrates its market relevance. While this doesn't generate revenue, it reinforces trust among new users and aids in securing appropriate project funding.\n\n\nUser Feedback\nFeedback, especially issues and bug reports, is invaluable. It steers us towards a more resilient and efficient product. This, too, isn't a revenue source but is pivotal for our project's evolution.\n\n\nAnonymous Telemetry Insights\nUsers who keep anonymous telemetry enabled, help us make data informed decisions in refining and enhancing Netdata. This isn't a revenue stream, but knowing which features are used and how, contributes in building a better product for everyone.\n\n\nWe don't monetize, directly or indirectly, users' or \"device heuristics\" data. Any data collected from community members are exclusively used for the purposes stated above.\nNetdata grows financially when technology intensive organizations and operators, need - due to regulatory or business requirements - the entire Netdata suite on-prem or private, bundled with top-tier support. It is a win-win case for all parties involved: these companies get a battle tested, robust and reliable solution, while the broader community that helps us build this product, enjoys it at no cost.\n  \n\n📖 Documentation\nNetdata's documentation is available at Netdata Learn.\nThis site also hosts a number of guides to help newer users better understand how\nto collect metrics, troubleshoot via charts, export to external databases, and more.\n🎉 Community\n\n  \n  \n  \n\nNetdata is an inclusive open-source project and community. Please read our Code of Conduct.\nJoin the Netdata community:\n\nChat with us and other community members on Discord.\nStart a discussion on GitHub discussions.\nOpen a topic to our community forums.\n\n\nMeet Up 🧑‍🤝‍🧑🧑‍🤝‍🧑🧑‍🤝‍🧑\nThe Netdata team and community members have regular online meetups.\nYou are welcome to join us!\nClick here for the schedule.\n\nYou can also find Netdata on:\nTwitter | YouTube | Reddit | LinkedIn | StackShare | Product Hunt | Repology | Facebook\n🙏 Contribute\n\n  \n\nContributions are essential to the success of open-source projects. In other words, we need your help to keep Netdata great!\nWhat is a contribution? All the following are highly valuable to Netdata:\n\n\nLet us know of the best-practices you believe should be standardized\nNetdata should out-of-the-box detect as many infrastructure issues as possible. By sharing your knowledge and experiences, you help us build a monitoring solution that has baked into it all the best-practices about infrastructure monitoring.\n\n\nLet us know if Netdata is not perfect for your use case\nWe aim to support as many use cases as possible and your feedback can be invaluable. Open a GitHub issue, or start a GitHub discussion about it, to discuss how you want to use Netdata and what you need.\nAlthough we can't implement everything imaginable, we try to prioritize development on use-cases that are common to our community, are in the same direction we want Netdata to evolve and are aligned with our roadmap.\n\n\nSupport other community members\nJoin our community on GitHub, Discord and Reddit. Generally, Netdata is relatively easy to set up and configure, but still people may need a little push in the right direction to use it effectively. Supporting other members is a great contribution by itself!\n\n\nAdd or improve integrations you need\nIntegrations tend to be easier and simpler to develop. If you would like to contribute your code to Netdata, we suggest that you start with the integrations you need, which Netdata does not currently support.\n\n\nGeneral information about contributions:\n\nCheck our Security Policy.\nFound a bug? Open a GitHub issue.\nRead our Contributing Guide, which contains all the information you need to contribute to Netdata, such as improving our documentation, engaging in the community, and developing new features. We've made it as frictionless as possible, but if you need help, just ping us on our community forums!\n\nPackage maintainers should read the guide on building Netdata from source for\ninstructions on building each Netdata component from the source and preparing a package.\nLicense\nThe Netdata ecosystem is comprised of three key components:\n\n\nNetdata Agent: The heart of the Netdata ecosystem, the Netdata Agent is an open-source tool that must be installed on all systems monitored by Netdata. It offers a wide range of essential features, including data collection via various plugins, an embedded high-performance time-series database (dbengine), unsupervised anomaly detection powered by edge-trained machine learning, alerting and notifications, as well as query and scoring engines with associated APIs. Additionally, it supports exporting data to third-party monitoring systems, among other capabilities.\nThe Netdata Agent is released under the GPLv3+ license and redistributes several other open-source tools and libraries, which are listed in the Netdata Agent third-party licenses.\n\n\nNetdata Cloud: A commercial, closed-source component, Netdata Cloud enhances the capabilities of the open-source Netdata Agent by providing horizontal scalability, centralized alert notification dispatch (including a mobile app), user management, role-based access control, and other enterprise-grade features. It is available both as a SaaS solution and for on-premises deployment, with a free-to-use community tier also offered.\n\n\nNetdata UI: The Netdata UI is closed-source, and handles all visualization and dashboard functionalities related to metrics, logs and other collected data, as well as the central configuration and management of the Netdata ecosystem. It serves both the Netdata Agent and Netdata Cloud. The Netdata UI is distributed in binary form with the Netdata Agent and is publicly accessible via a CDN, licensed under the Netdata Cloud UI License 1 (NCUL1). It integrates third-party open-source components, detailed in the Netdata UI third-party licenses.\n\n\nThe binary installation packages provided by Netdata include the Netdata Agent and the Netdata UI. Since the Netdata Agent is open-source, it is frequently packaged by third parties (e.g. Linux Distributions) excluding the closed-source components (Netdata UI is not included). While their packages can still be useful in providing the necessary back-ends and the APIs of a fully functional monitoring solution, we recommend using the installation packages we provide to experience the full feature set of Netdata."
}
{
    "repo_name": "mlx-swift-examples",
    "watchers": "14",
    "forks": "96",
    "stars": "882",
    "languages": {},
    "about_info": "Examples using MLX Swift",
    "repo_url": "https://github.com/ml-explore/mlx-swift-examples",
    "readme_content": "MLX Swift Examples\nExample MLX Swift programs.\n\n\nMNISTTrainer: An example that runs on\nboth iOS and macOS that downloads MNIST training data and trains a\nLeNet.\n\n\nLLMEval: An example that runs on both iOS\nand macOS that downloads an LLM and tokenizer from Hugging Face and\ngenerates text from a given prompt.\n\n\nLinearModelTraining: An example that\ntrains a simple linear model.\n\n\nStableDiffusionExample: An\nexample that runs on both iOS and macOS that downloads a stable diffusion model\nfrom Hugging Face and  and generates an image from a given prompt.\n\n\nllm-tool: A command line tool for generating text\nusing a variety of LLMs available on the Hugging Face hub.\n\n\nimage-tool: A command line tool for generating images\nusing a stable diffusion model from Hugging Face.\n\n\nmnist-tool: A command line tool for training a\na LeNet on MNIST.\n\n\nRunning\nThe application and command line tool examples can be run from Xcode or from\nthe command line:\n./mlx-run llm-tool --prompt \"swift programming language\"\n\nSee also:\n\nMLX troubleshooting\n\nInstallation of MLXLLM and MLXMNIST libraries\nThe MLXLLM, MLXMNIST and StableDiffusion libraries in the example repo are available\nas Swift Packages.\nAdd the following dependency to your Package.swift\n.package(url: \"https://github.com/ml-explore/mlx-swift-examples/\", branch: \"main\"),\nThen add one library or both libraries to the target as a dependency.\n.target(\n    name: \"YourTargetName\",\n    dependencies: [\n        .product(name: \"LLM\", package: \"mlx-swift-examples\")\n    ]),\nAlternatively, add https://github.com/ml-explore/mlx-swift-examples/ to the Project Dependencies and set the Dependency Rule to Branch and main in Xcode."
}
{
    "repo_name": "maybe",
    "watchers": "155",
    "forks": "2.3k",
    "stars": "29.8k",
    "languages": {},
    "about_info": "The OS for your personal finances",
    "repo_url": "https://github.com/maybe-finance/maybe",
    "readme_content": "(Note: The image above is a mockup of what we're working towards. We're rapidly approaching the functionality shown, but not all of the parts are ready just yet.)\nMaybe: The OS for your personal finances\nGet\ninvolved: Discord • Website • Issues\nIf you're looking for the previous React codebase, you can find it\nat maybe-finance/maybe-archive.\nBackstory\nWe spent the better part of 2021/2022 building a personal finance + wealth\nmanagement app called, Maybe. Very full-featured, including an \"Ask an Advisor\"\nfeature which connected users with an actual CFP/CFA to help them with their\nfinances (all included in your subscription).\nThe business end of things didn't work out, and so we shut things down mid-2023.\nWe spent the better part of $1,000,000 building the app (employees +\ncontractors, data providers/services, infrastructure, etc.).\nWe're now reviving the product as a fully open-source project. The goal is to\nlet you run the app yourself, for free, and use it to manage your own finances\nand eventually offer a hosted version of the app for a small monthly fee.\nMaybe Hosting\nThere are 3 primary ways to use the Maybe app:\n\nManaged (easiest) - coming soon...\nOne-click deploy\nSelf-host with Docker\n\nLocal Development Setup\nIf you are trying to self-host the Maybe app, stop here. You\nshould read this guide to get started.\nThe instructions below are for developers to get started with contributing to the app.\nRequirements\n\nRuby 3.3.4\nPostgreSQL >9.3 (ideally, latest stable version)\n\nAfter cloning the repo, the basic setup commands are:\ncd maybe\ncp .env.example .env\nbin/setup\nbin/dev\n\n# Optionally, load demo data\nrake demo_data:reset\nAnd visit http://localhost:3000 to see the app. You can use the following\ncredentials to log in (generated by DB seed):\n\nEmail: user@maybe.local\nPassword: password\n\nFor further instructions, see guides below.\nMulti-currency support\nIf you'd like multi-currency support, there are a few extra steps to follow.\n\nSign up for an API key at Synth. It's a Maybe\nproduct and the free plan is sufficient for basic multi-currency support.\nAdd your API key to your .env file.\n\nSetup Guides\nDev Container (optional)\nThis is 100% optional and meant for devs who don't want to worry about\ninstalling requirements manually for their platform. You can\nfollow this guide\nto learn more about Dev Containers.\nIf you run into could not connect to server errors, you may need to change\nyour .env's DB_HOST environment variable value to db to point to the\nPostgres container.\nMac\nPlease visit\nour Mac dev setup guide.\nLinux\nPlease visit\nour Linux dev setup guide.\nWindows\nPlease visit\nour Windows dev setup guide.\nTesting Emails\nIn development, we use letter_opener to automatically open emails in your\nbrowser. When an email sends locally, a new browser tab will open with a\npreview.\nContributing\nBefore contributing, you'll likely find it helpful\nto understand context and general vision/direction.\nOnce you've done that, please visit\nour contributing guide\nto get started!\nRepo Activity\n\nCopyright & license\nMaybe is distributed under\nan AGPLv3 license. \"\nMaybe\" is a trademark of Maybe Finance, Inc."
}
{
    "repo_name": "bitcoin",
    "watchers": "4k",
    "forks": "36.2k",
    "stars": "78.6k",
    "languages": {},
    "about_info": "Bitcoin Core integration/staging tree",
    "repo_url": "https://github.com/bitcoin/bitcoin",
    "readme_content": "Bitcoin Core integration/staging tree\nhttps://bitcoincore.org\nFor an immediately usable, binary version of the Bitcoin Core software, see\nhttps://bitcoincore.org/en/download/.\nWhat is Bitcoin Core?\nBitcoin Core connects to the Bitcoin peer-to-peer network to download and fully\nvalidate blocks and transactions. It also includes a wallet and graphical user\ninterface, which can be optionally built.\nFurther information about Bitcoin Core is available in the doc folder.\nLicense\nBitcoin Core is released under the terms of the MIT license. See COPYING for more\ninformation or see https://opensource.org/licenses/MIT.\nDevelopment Process\nThe master branch is regularly built (see doc/build-*.md for instructions) and tested, but it is not guaranteed to be\ncompletely stable. Tags are created\nregularly from release branches to indicate new official, stable release versions of Bitcoin Core.\nThe https://github.com/bitcoin-core/gui repository is used exclusively for the\ndevelopment of the GUI. Its master branch is identical in all monotree\nrepositories. Release branches and tags do not exist, so please do not fork\nthat repository unless it is for development reasons.\nThe contribution workflow is described in CONTRIBUTING.md\nand useful hints for developers can be found in doc/developer-notes.md.\nTesting\nTesting and code review is the bottleneck for development; we get more pull\nrequests than we can review and test on short notice. Please be patient and help out by testing\nother people's pull requests, and remember this is a security-critical project where any mistake might cost people\nlots of money.\nAutomated Testing\nDevelopers are strongly encouraged to write unit tests for new code, and to\nsubmit new unit tests for old code. Unit tests can be compiled and run\n(assuming they weren't disabled during the generation of the build system) with: ctest. Further details on running\nand extending unit tests can be found in /src/test/README.md.\nThere are also regression and integration tests, written\nin Python.\nThese tests can be run (if the test dependencies are installed) with: test/functional/test_runner.py\nThe CI (Continuous Integration) systems make sure that every pull request is built for Windows, Linux, and macOS,\nand that unit/sanity tests are run automatically.\nManual Quality Assurance (QA) Testing\nChanges should be tested by somebody other than the developer who wrote the\ncode. This is especially important for large or high-risk changes. It is useful\nto add a test plan to the pull request description if testing the changes is\nnot straightforward.\nTranslations\nChanges to translations as well as new translations can be submitted to\nBitcoin Core's Transifex page.\nTranslations are periodically pulled from Transifex and merged into the git repository. See the\ntranslation process for details on how this works.\nImportant: We do not accept translation changes as GitHub pull requests because the next\npull from Transifex would automatically overwrite them again."
}
{
    "repo_name": "si",
    "watchers": "21",
    "forks": "117",
    "stars": "1.3k",
    "languages": {},
    "about_info": "The System Initiative software",
    "repo_url": "https://github.com/systeminit/si",
    "readme_content": "System Initiative\n\n\n\nThis is a monolithic repository containing the System Initiative software.\nAbout\nSystem Initiative is the future of DevOps Automation. It is an Intuitive, Powerful, and Collaborative replacement for Infrastructure as Code.\nTo learn more, read our \"System Initiative is the Future\" blog post.\nQuickstart\nFollow the Local Development Setup instructions below.\nWe are working on and investigating more way(s) to try out System Initiative in the future.\nLocal Development Setup\nRunning the System Initiative software locally can be done in a variety of ways, but this abbreviated section will focus on a single method for getting your environment ready to run the stack.\nFor more information and options on running SI locally, see the development environment section of the docs.\n(1) Choose a Supported Platform\nLet's start by choosing an officially supported platform.\n\n\n\nArchitecture\nOperating System\n\n\n\n\nx86_64 (amd64)\nmacOS, Linux, WSL2 (Windows 10/11)\n\n\naarch64 (arm64)\nmacOS, Linux, WSL2 (Windows 10/11)\n\n\n\nPlatform Notes:\n\nOn Apple Silicon systems (i.e. macOS aarch64 (arm64)), Rosetta 2 must be installed (install it with softwareupdate --install-rosetta)\nNixOS requires docker to be installed and Flakes to be enabled (see the development environment section of the docs for more information)\nLinux with MUSL instead of GNU (e.g. Alpine Linux) is untested\nSystemd may need to be enabled on WSL2\n\n(2) Install Dependencies\nInstall dependencies on your chosen platform.\n\n1) nix with flakes enabled version >= 2.18.1 installed\n2) docker from Docker Desktop or Docker Engine\n3a) direnv version >= 2.30 installed\n3b) direnv hooked into your shell\n\nFor nix, we highly recommend using the Determinate Nix Installer.\nFor docker, the Docker Desktop version corresponding to your native architecture should be used.\nWSL2 users should be able to use either Docker Desktop for WSL2 or Docker Engine inside the WSL2 VM.\nFor direnv, you can install it with your package manager of choice.\nHowever, if you're unsure which installation method to use or your package manager does not provide a compatible version, you can use nix itself (e.g. nix profile install nixpkgs#direnv).\nTipWe recommend using the upstream docs for hooking direnv into your shell, but here is an example on how to do it on a system where zsh is the default shell.\nIn this example, the following is added to the end of ~/.zshrc.\nif [ $(command -v direnv) ]; then\n   eval \"$(direnv hook zsh)\"\nfi\n\n(3) Enter the Repository Directory\nAll commands need to be run from the nix environment.\nSince direnv is installed and hooked into your shell, you can cd into the repository and nix will bootstrap the environment for you using the flake.\nWarningYou may notice a large download of dependencies when entering the repository for the first time.\n\n(4) (Optional) Configure Docker\nDocker Hub authentication is not strictly needed if you only access public docker images, but to avoid being rate-limited when qualifying images, we recommend authenticating with the docker CLI.\ndocker login\n(5) Running the Stack\nWe use buck2 to run the stack, run and build individual services and libraries, perform lints and tests, etc.\nBefore continuing, you should stop any locally running services to avoid conflicting ports with the stack.\nSome of the services that will run include, but are not limited to the following: PostgreSQL, NATS, Jaeger and OpenTelemetry.\nCheck if you are ready to run the stack before continuing.\nbuck2 run dev:healthcheck\nYou may notice some checks related to resource limits.\nOn macOS and in WSL2 in particular, we recommend significantly increasing the file descriptor limit for buck2 to work as intended (e.g. ulimit -n 10240).\nPlease note: the new file descriptor limit may not persist to future sessions.\nOnce ready, we can build relevant services and run the entire stack locally.\nPlease note: if you have used SI before, the following command will delete all contents of the database.\nReach out to us on Discord if you have any questions.\nbuck2 run dev:up\nOnce Tilt starts, you can check on the status of all services by accessing the UI through the given port on your local host (e.g. http://localhost:10350/).\nEvery service should eventually have a green checkmark next to them, which ensures that they are in \"ready\" states.\nWarningDatabase migrations may take some time to complete.\n\nIf you would like to learn more on what's running, check out the docs.\nIn our documentation, you can also learn more about running the stack locally and a deeper dive into system requirements.\n(6) Troubleshooting in Tilt\nIf some services failed to start, you can restart them on the Tilt dashboard.\n\nA backend service fails (e.g. sdf): restart them in the following order: forklift, veritech, rebaser, pinga, sdf\nA frontend service fails (e.g. web): restart the service individually\nA dependent service fails (e.g. PostgreSQL): tear down the stack and restart\n\n(7) Tearing Down the Stack\nThe following command will stop all running services and containers.\nIt will also remove the containers and, consequentially, the data held in them.\nbuck2 run dev:down\nAlternatively, if you wish to keep your data for later use, you can stop the containers without removing them.\nbuck2 run dev:stop\nWhere Do I Learn More?\nFor more information on how to use and develop the System Initiative software, talk to us on\nour Discord and see the DOCS.\nHow Can I Contribute?\nTo start, we recommend reading the Open Source and Contributing sources below.\nThey provide information on licensing, contributor rights, and more.\nAfter that, navigate to the contributing guide to get started.\nOpen Source\nThis repository contains the System Initiative software, covered under the Apache License 2.0, except where noted (any System Initiative logos or trademarks are not covered under the Apache License, and should be explicitly noted by a LICENSE file.)\nSystem Initiative is a product produced from this open source software, exclusively by System Initiative, Inc. It is distributed under our commercial terms.\nOthers are allowed to make their own distribution of the software, but they cannot use any of the System Initiative trademarks, cloud services, etc.\nWe explicitly grant permission for you to make a build that includes our trademarks while developing the System Initiative software itself. You may not publish or share the build, and you may not use that build to run System Initiative software for any other purpose.\nYou can learn more about the System Initiative software and Open Source in our FAQ.\nContributing\nThe System Initiative software is Open Source under the Apache License 2.0, and is the copyright of its contributors. If you would like to contribute to the software, you must:\n\nRead the Contributors file.\nAgree to the terms by having a commit in your pull request \"signing\" the file by adding your name and GitHub handle on a new line at the bottom of the file.\nMake sure your commits Author metadata matches the name and handle you added to the file.\n\nThis ensures that users, distributors, and other contributors can rely on all the software related to System Initiative being contributed under the terms of the License. No contributions will be accepted without following this process."
}
{
    "repo_name": "nextra",
    "watchers": "36",
    "forks": "1.2k",
    "stars": "11.4k",
    "languages": {},
    "about_info": "Simple, powerful and flexible site generation framework with everything you love from Next.js.",
    "repo_url": "https://github.com/shuding/nextra",
    "readme_content": "Nextra\nSimple, powerful and flexible site generation framework with everything you love\nfrom Next.js.\nDocumentation\nhttps://nextra.site\nDevelopment\nInstallation\nThe Nextra repository uses PNPM Workspaces and\nTurborepo. To install dependencies, run\npnpm install in the project root directory.\nBuild Nextra Core\ncd packages/nextra\npnpm build\nWatch mode: pnpm dev\nBuild Nextra Theme\ncd packages/nextra-theme-docs\npnpm build\n\n\n\nCommand\nDescription\n\n\n\n\npnpm dev\nWatch mode\n\n\npnpm dev:layout\nWatch mode (layout only)\n\n\npnpm dev:tailwind\nWatch mode (style only)\n\n\n\nDevelopment\nYou can also debug them together with a website locally. For instance, to start\nexamples/docs locally, run\ncd examples/docs\npnpm dev\nAny change to example/docs will be re-rendered instantly.\nIf you update the core or theme packages, a rebuild is required. Or you can use\nthe watch mode for both nextra and the theme in separated terminals.\nSponsors"
}
{
    "repo_name": "Pake",
    "watchers": "168",
    "forks": "5k",
    "stars": "28.9k",
    "languages": {},
    "about_info": "🤱🏻 Turn any webpage into a desktop app with Rust. 🤱🏻 利用 Rust 轻松构建轻量级多端桌面应用",
    "repo_url": "https://github.com/tw93/Pake",
    "readme_content": "English | 简体中文 | 日本語\n\n    \n\nPake\nTurn any webpage into a desktop app with Rust with ease.\n\n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n\nPake supports Mac, Windows, and Linux. Check out README for Popular Packages, Command-Line Packaging, and Customized Development information. Feel free to share your suggestions in Discussions.\nFeatures\n\n🎐 Nearly 20 times smaller than an Electron package (around 5M!)\n🚀 With Rust Tauri, Pake is much more lightweight and faster than JS-based frameworks.\n📦 Battery-included package — shortcut pass-through, immersive windows, and minimalist customization.\n👻 Pake is just a simple tool — replace the old bundle approach with Tauri (though PWA is good enough).\n\nPopular Packages\n\n    \n        WeRead\n            Mac\n            Windows\n            Linux\n        \n        Twitter\n            Mac\n            Windows\n            Linux\n        \n    \n    \n        \n        \n    \n    \n        ChatGPT\n            Mac\n            Windows\n            Linux\n        \n        Poe\n            Mac\n            Windows\n            Linux\n        \n    \n    \n        \n        \n    \n    \n      YouTube Music\n            Mac\n            Windows\n            Linux\n      \n      YouTube\n            Mac\n            Windows\n            Linux\n      \n    \n    \n        \n        \n    \n    \n        LiZhi\n            Mac\n            Windows\n            Linux\n        \n        ProgramMusic\n            Mac\n            Windows\n            Linux\n        \n    \n    \n        \n        \n    \n    \n        Qwerty\n            Mac\n            Windows\n            Linux\n        \n        CodeRunner\n            Mac\n            Windows\n            Linux\n        \n    \n    \n        \n        \n    \n        \n        Flomo\n            Mac\n            Windows\n            Linux\n        \n        XiaoHongShu\n            Mac\n            Windows\n            Linux\n        \n    \n    \n        \n        \n    \n\n\n🏂 You can download more applications from Releases. Click here to expand the shortcuts reference!\n\n\n\n\nMac\nWindows/Linux\nFunction\n\n\n\n\n⌘ + [\nCtrl + ←\nReturn to the previous page\n\n\n⌘ + ]\nCtrl + →\nGo to the next page\n\n\n⌘ + ↑\nCtrl + ↑\nAuto scroll to top of page\n\n\n⌘ + ↓\nCtrl + ↓\nAuto scroll to bottom of page\n\n\n⌘ + r\nCtrl + r\nRefresh Page\n\n\n⌘ + w\nCtrl + w\nHide window, not quite\n\n\n⌘ + -\nCtrl + -\nZoom out the page\n\n\n⌘ + +\nCtrl + +\nZoom in the page\n\n\n⌘ + =\nCtrl + =\nZoom in the Page\n\n\n⌘ + 0\nCtrl + 0\nReset the page zoom\n\n\n\nIn addition, double-click the title bar to switch to full-screen mode. For Mac users, you can also use the gesture to go to the previous or next page and drag the title bar to move the window.\n\nBefore starting\n\nFor beginners: Play with Popular Packages to find out Pake's capabilities, or try to pack your application with GitHub Actions. Don't hesitate to reach for assistance at Discussion!\nFor developers: “Command-Line Packaging” supports macOS fully. For Windows/Linux users, it requires some tinkering. Configure your environment before getting started.\nFor hackers: For people who are good at both front-end development and Rust, how about customizing your apps' function more with the following Customized Development?\n\nCommand-Line Packaging\n\nPake provides a command line tool, making the flow of package customization quicker and easier. See documentation for more information.\n# Install with npm\nnpm install -g pake-cli\n\n# Command usage\npake url [OPTIONS]...\n\n# Feel free to play with Pake! It might take a while to prepare the environment the first time you launch Pake.\npake https://weekly.tw93.fun --name Weekly --hide-title-bar\n\nIf you are new to the command line, you can compile packages online with GitHub Actions. See the Tutorial for more information.\nDevelopment\nPrepare your environment before starting. Make sure you have Rust >=1.63 and Node >=16 (e.g., 16.18.1) installed on your computer. For installation guidance, see Tauri documentation.\nIf you are unfamiliar with these, it is better to try out the above tool to pack with one click.\n# Install Dependencies\nnpm i\n\n# Local development [Right-click to open debug mode.]\nnpm run dev\n\n# Pack application\nnpm run build\nAdvanced Usage\n\nYou can refer to the codebase structure before working on Pake, which will help you much in development.\nModify the url and productName fields in the pake.json file under the src-tauri directory, the \"domain\" field in the tauri.config.json file needs to be modified synchronously, as well as the icon and identifier fields in the tauri.xxx.conf.json file. You can select an icon from the icons directory or download one from macOSicons to match your product needs.\nFor configurations on window properties, you can modify the pake.json file to change the value of width, height, fullscreen (or not), resizable (or not) of the windows property. To adapt to the immersive header on Mac, change hideTitleBar to true, look for the Header element, and add the padding-top property.\nFor advanced usages such as style rewriting, advertisement removal, JS injection, container message communication, and user-defined shortcut keys, see Advanced Usage of Pake.\n\nDevelopers\nPake's development can not be without these Hackers. They contributed a lot of capabilities for Pake. Also, welcome to follow them! ❤️\n\n\n\n    \n        \n            \n            \n            Tw93\n        \n    \n    \n        \n            \n            \n            Tlntin\n        \n    \n    \n        \n            \n            \n            Santree\n        \n    \n    \n        \n            \n            \n            Pan93412\n        \n    \n    \n        \n            \n            \n            Volare\n        \n    \n    \n        \n            \n            \n            Bryan Lee\n        \n    \n    \n        \n            \n            \n            Essesoul\n        \n    \n\n    \n        \n            \n            \n            Jerry Zhou\n        \n    \n    \n        \n            \n            \n            Aiello\n        \n    \n    \n        \n            \n            \n            Horus\n        \n    \n    \n        \n            \n            \n            Pake Actions\n        \n    \n    \n        \n            \n            \n            Ikko Eltociear Ashimine\n        \n    \n    \n        \n            \n            \n            Steam\n        \n    \n    \n        \n            \n            \n            孟世博\n        \n    \n\n    \n        \n            \n            \n            2nthony\n        \n    \n    \n        \n            \n            \n            Null\n        \n    \n    \n        \n            \n            \n            Abu Taher Siddik\n        \n    \n    \n        \n            \n            \n            An Li\n        \n    \n    \n        \n            \n            \n            Ayaka Neko\n        \n    \n    \n        \n            \n            \n            Dengju Deng\n        \n    \n    \n        \n            \n            \n            Fechin\n        \n    \n\n    \n        \n            \n            \n            Imgbot\n        \n    \n    \n        \n            \n            \n            Jiaqi Gu\n        \n    \n    \n        \n            \n            \n            Matt Bajorek\n        \n    \n    \n        \n            \n            \n            Milo\n        \n    \n    \n        \n            \n            \n            Po Chen\n        \n    \n    \n        \n            \n            \n            Qitianjia\n        \n    \n    \n        \n            \n            \n            Null\n        \n    \n\n    \n        \n            \n            \n            Hyzhao\n        \n    \n    \n        \n            \n            \n            Null\n        \n    \n    \n        \n            \n            \n            Liudonghua\n        \n    \n    \n        \n            \n            \n            Liusishan\n        \n    \n    \n        \n            \n            \n            Ranger\n        \n    \n    \n        \n            \n            \n            贺天卓\n        \n    \n\n\nFrequently Asked Questions\n\nRight-clicking on an image element in the page to open the menu and select download image or other events does not work (common in MacOS systems). This issue is due to the MacOS built-in webview not supporting this feature.\n\nSupport\n\nI have two cats, TangYuan and Coke. If you think Pake delights your life, you can feed them some canned food 🥩.\nIf you like Pake, you can star it on GitHub. Also, welcome to recommend Pake to your friends.\nYou can follow my Twitter to get the latest news of Pake or join our Telegram chat group.\nI hope that you enjoy playing with it. Let us know if you find a website that would be great for a Mac App!"
}
{
    "repo_name": "llama-stack",
    "watchers": "30",
    "forks": "242",
    "stars": "2.4k",
    "languages": {},
    "about_info": "Model components of the Llama Stack APIs",
    "repo_url": "https://github.com/meta-llama/llama-stack",
    "readme_content": "Llama Stack\n\n\nThis repository contains the Llama Stack API specifications as well as API Providers and Llama Stack Distributions.\nThe Llama Stack defines and standardizes the building blocks needed to bring generative AI applications to market. These blocks span the entire development lifecycle: from model training and fine-tuning, through product evaluation, to building and running AI agents in production. Beyond definition, we are building providers for the Llama Stack APIs. These were developing open-source versions and partnering with providers, ensuring developers can assemble AI solutions using consistent, interlocking pieces across platforms. The ultimate goal is to accelerate innovation in the AI space.\nThe Stack APIs are rapidly improving, but still very much work in progress and we invite feedback as well as direct contributions.\nAPIs\nThe Llama Stack consists of the following set of APIs:\n\nInference\nSafety\nMemory\nAgentic System\nEvaluation\nPost Training\nSynthetic Data Generation\nReward Scoring\n\nEach of the APIs themselves is a collection of REST endpoints.\nAPI Providers\nA Provider is what makes the API real -- they provide the actual implementation backing the API.\nAs an example, for Inference, we could have the implementation be backed by open source libraries like [ torch | vLLM | TensorRT ] as possible options.\nA provider can also be just a pointer to a remote REST service -- for example, cloud providers or dedicated inference providers could serve these APIs.\nLlama Stack Distribution\nA Distribution is where APIs and Providers are assembled together to provide a consistent whole to the end application developer. You can mix-and-match providers -- some could be backed by local code and some could be remote. As a hobbyist, you can serve a small model locally, but can choose a cloud provider for a large model. Regardless, the higher level APIs your app needs to work with don't need to change at all. You can even imagine moving across the server / mobile-device boundary as well always using the same uniform set of APIs for developing Generative AI applications.\nSupported Llama Stack Implementations\nAPI Providers\n\n\n\nAPI Provider Builder\nEnvironments\nAgents\nInference\nMemory\nSafety\nTelemetry\n\n\n\n\nMeta Reference\nSingle Node\n✔️\n✔️\n✔️\n✔️\n✔️\n\n\nFireworks\nHosted\n✔️\n✔️\n✔️\n\n\n\n\nAWS Bedrock\nHosted\n\n✔️\n\n✔️\n\n\n\nTogether\nHosted\n✔️\n✔️\n\n✔️\n\n\n\nOllama\nSingle Node\n\n✔️\n\n\n\n\n\nTGI\nHosted and Single Node\n\n✔️\n\n\n\n\n\nChroma\nSingle Node\n\n\n✔️\n\n\n\n\nPG Vector\nSingle Node\n\n\n✔️\n\n\n\n\nPyTorch ExecuTorch\nOn-device iOS\n✔️\n✔️\n\n\n\n\n\n\nDistributions\n\n\n\nDistribution Provider\nDocker\nInference\nMemory\nSafety\nTelemetry\n\n\n\n\nMeta Reference\nLocal GPU, Local CPU\n✔️\n✔️\n✔️\n✔️\n\n\nDell-TGI\nLocal TGI + Chroma\n✔️\n✔️\n✔️\n✔️\n\n\n\nInstallation\nYou can install this repository as a package with pip install llama-stack\nIf you want to install from source:\nmkdir -p ~/local\ncd ~/local\ngit clone git@github.com:meta-llama/llama-stack.git\n\nconda create -n stack python=3.10\nconda activate stack\n\ncd llama-stack\n$CONDA_PREFIX/bin/pip install -e .\nThe Llama CLI\nThe llama CLI makes it easy to work with the Llama Stack set of tools, including installing and running Distributions, downloading models, studying model prompt formats, etc. Please see the CLI reference for details. Please see the Getting Started guide for running a Llama Stack server.\nLlama Stack Client SDK\nCheck out our client SDKs for connecting to Llama Stack server in your preferred language, you can choose from python, node, swift, and kotlin programming languages to quickly build your applications."
}
{
    "repo_name": "exo",
    "watchers": "69",
    "forks": "396",
    "stars": "7.5k",
    "languages": {},
    "about_info": "Run your own AI cluster at home with everyday devices 📱💻 🖥️⌚",
    "repo_url": "https://github.com/exo-explore/exo",
    "readme_content": "exo: Run your own AI cluster at home with everyday devices. Maintained by exo labs.\n\nDiscord | Telegram | X\n\n\n\n\n\n\nForget expensive NVIDIA GPUs, unify your existing devices into one powerful GPU: iPhone, iPad, Android, Mac, Linux, pretty much any device!\n\n  Update: exo is hiring. See here for more details.\n\nGet Involved\nexo is experimental software. Expect bugs early on. Create issues so they can be fixed. The exo labs team will strive to resolve issues quickly.\nWe also welcome contributions from the community. We have a list of bounties in this sheet.\nFeatures\nWide Model Support\nexo supports different models including LLaMA (MLX and tinygrad), Mistral, LlaVA, Qwen and Deepseek.\nDynamic Model Partitioning\nexo optimally splits up models based on the current network topology and device resources available. This enables you to run larger models than you would be able to on any single device.\nAutomatic Device Discovery\nexo will automatically discover other devices using the best method available. Zero manual configuration.\nChatGPT-compatible API\nexo provides a ChatGPT-compatible API for running models. It's a one-line change in your application to run models on your own hardware using exo.\nDevice Equality\nUnlike other distributed inference frameworks, exo does not use a master-worker architecture. Instead, exo devices connect p2p. As long as a device is connected somewhere in the network, it can be used to run models.\nExo supports different partitioning strategies to split up a model across devices. The default partitioning strategy is ring memory weighted partitioning. This runs an inference in a ring where each device runs a number of model layers proportional to the memory of the device.\n\n    \n        \n    \n\nInstallation\nThe current recommended way to install exo is from source.\nPrerequisites\n\nPython>=3.12.0 is required because of issues with asyncio in previous versions.\nLinux (with NVIDIA card):\n\nNVIDIA driver (test with nvidia-smi)\nCUDA (https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#cuda-cross-platform-installation) (test with nvcc --version)\ncuDNN (https://developer.nvidia.com/cudnn-downloads) (test with link)\n\n\n\nFrom source\ngit clone https://github.com/exo-explore/exo.git\ncd exo\npip install .\n# alternatively, with venv\nsource install.sh\nTroubleshooting\n\nIf running on Mac, MLX has an install guide with troubleshooting steps.\n\nPerformance\n\nThere are a number of things users have empirically found to improve performance on Apple Silicon Macs:\n\n\nUpgrade to the latest version of MacOS 15.\nRun ./configure_mlx.sh. This runs commands to optimize GPU memory allocation on Apple Silicon Macs.\n\nDocumentation\nExample Usage on Multiple MacOS Devices\nDevice 1:\npython3 main.py\nDevice 2:\npython3 main.py\nThat's it! No configuration required - exo will automatically discover the other device(s).\nexo starts a ChatGPT-like WebUI (powered by tinygrad tinychat) on http://localhost:8000\nFor developers, exo also starts a ChatGPT-compatible API endpoint on http://localhost:8000/v1/chat/completions. Example with curls:\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llama-3.1-8b\",\n     \"messages\": [{\"role\": \"user\", \"content\": \"What is the meaning of exo?\"}],\n     \"temperature\": 0.7\n   }'\ncurl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n     \"model\": \"llava-1.5-7b-hf\",\n     \"messages\": [\n      {\n        \"role\": \"user\",\n        \"content\": [\n          {\n            \"type\": \"text\",\n            \"text\": \"What are these?\"\n          },\n          {\n            \"type\": \"image_url\",\n            \"image_url\": {\n              \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n            }\n          }\n        ]\n      }\n    ],\n     \"temperature\": 0.0\n   }'\nExample Usage on Multiple Heterogenous Devices (MacOS + Linux)\nDevice 1 (MacOS):\npython3 main.py --inference-engine tinygrad\nHere we explicitly tell exo to use the tinygrad inference engine.\nDevice 2 (Linux):\npython3 main.py\nLinux devices will automatically default to using the tinygrad inference engine.\nYou can read about tinygrad-specific env vars here. For example, you can configure tinygrad to use the cpu by specifying CLANG=1.\nDebugging\nEnable debug logs with the DEBUG environment variable (0-9).\nDEBUG=9 python3 main.py\nFor the tinygrad inference engine specifically, there is a separate DEBUG flag TINYGRAD_DEBUG that can be used to enable debug logs (1-6).\nTINYGRAD_DEBUG=2 python3 main.py\nKnown Issues\n\nOn some versions of MacOS/Python, certificates are not installed properly which can lead to SSL errors (e.g. SSL error with huggingface.co). To fix this, run the Install Certificates command, usually:\n\n/Applications/Python 3.x/Install Certificates.command\n\n🚧 As the library is evolving so quickly, the iOS implementation has fallen behind Python. We have decided for now not to put out the buggy iOS version and receive a bunch of GitHub issues for outdated code. We are working on solving this properly and will make an announcement when it's ready. If you would like access to the iOS implementation now, please email alex@exolabs.net with your GitHub username explaining your use-case and you will be granted access on GitHub.\n\nInference Engines\nexo supports the following inference engines:\n\n✅ MLX\n✅ tinygrad\n🚧 llama.cpp\n\nNetworking Modules\n\n✅ GRPC\n🚧 Radio\n🚧 Bluetooth"
}
{
    "repo_name": "spotube",
    "watchers": "125",
    "forks": "1.2k",
    "stars": "29.2k",
    "languages": {},
    "about_info": "🎧 Open source Spotify client that doesn't require Premium nor uses Electron! Available for both desktop & mobile!",
    "repo_url": "https://github.com/KRTirtho/spotube",
    "readme_content": "An open source, cross-platform Spotify client compatible across multiple platforms\nutilizing Spotify's data API and YouTube, Piped.video or JioSaavn as an audio source,\neliminating the need for Spotify Premium\nBtw it's not just another Electron app 😉\n\n\n\n\n\n\n\n\n\n\n🌃 Features\n\n🚫 No ads, thanks to the use of public & free Spotify and YT Music APIs¹\n⬇️ Freely downloadable tracks\n🖥️ 📱 Cross-platform support\n🪶 Small size & less data usage\n🕵️ Anonymous/guest login\n🕒 Time synced lyrics\n✋ No telemetry, diagnostics or user data collection\n🚀 Native performance\n📖 Open source/libre software\n🔉 Playback control is done locally, not on the server\n\n¹ It is still recommended to support creators by engaging with their YouTube channels/Spotify tracks (or preferably by buying their merch/concert tickets/physical media).\n❌ Unsupported features\n\n🗣️ Spotify Shows & Podcasts: Shows and Podcasts will never be supported because the audio tracks are only available on Spotify and accessing them would require Spotify Premium.\n🎧 Spotify Listen Along: Coming soon!\n\n📜 ⬇️ Installation guide\nNew versions usually release every 3-4 months.\nThis handy table lists all the methods you can use to install Spotube:\n\n  \n    Platform\n    Package/Installation Method\n  \n  \n    Windows\n    \n      \n        \n      \n  \n  \n    MacOS\n    \n      \n        \n      \n    \n  \n  \n    Android\n    \n      \n        \n      \n      \n      \n        \n      \n      \n      \n        \n      \n    \n  \n  \n    Flatpak\n    \n      flatpak install com.github.KRTirtho.Spotube\n      \n        \n      \n    \n  \n  \n    AppImage\n    AppImage's lacking stability led to it's temporal removal. More information at #1082\n  \n  \n    Debian/Ubuntu\n    \n      \n        \n      \n      Then run: sudo apt install ./Spotube-linux-x86_64.deb\n    \n  \n  \n    Arch/Manjaro\n    \n      With pamac: sudo pamac install spotube-bin\n      With yay: yay -Sy spotube-bin\n    \n  \n  \n    Fedora/OpenSuse\n    \n      \n        \n      \n      For Fedora: sudo dnf install ./Spotube-linux-x86_64.rpm\n      For OpenSuse: sudo zypper in ./Spotube-linux-x86_64.rpm\n    \n  \n  \n    Linux (tarball)\n    \n      \n        \n      \n    \n  \n  \n    Macos - Homebrew\n    \nbrew tap krtirtho/apps\nbrew install --cask spotube   \n    \n  \n  \n    Windows - Chocolatey\n    \n      choco install spotube\n    \n  \n  \n    Windows - Scoop\n    \n      scoop bucket add extras\n      scoop install spotube\n    \n  \n  \n    Windows - WinGet\n    \n      winget install --id KRTirtho.Spotube\n    \n  \n\n🔄 Nightly Builds\nGrab the latest nightly builds of Spotube from the GitHub Releases.\n🕳️ Building from source\n\nYou can compile Spotube's source code by following these instructions.\n👥 The Spotube team\n\nKingkor Roy Tirtho - The Founder, Maintainer and Lead Developer\nRaptaG - The GitHub Moderator and Community Manager\nOwen Connor - The Cool Discord Moderator\nMeenbeese - The Android Developer\nPiotr Rogowski - The MacOS Developer\nRusty Apple - The Mysterious Unknown Guy\n\n💼 License\nSpotube is open source and licensed under the BSD-4-Clause License.\nIf you are concerned, you can read the reason of choosing this license.\n\n  \n    [Click to show] 🙏 Services/Package/Plugin Credits\n  \nServices\n\nFlutter - Flutter transforms the app development process. Build, test, and deploy beautiful mobile, web, desktop, and embedded apps from a single codebase\nSpotify API - The Spotify Web API is a RESTful API that provides access to Spotify data\nPiped - Piped is a privacy friendly alternative YouTube frontend, which is efficient and scalable by design.\nYouTube - YouTube is an American online video-sharing platform headquartered in San Bruno, California. Three former PayPal employees—Chad Hurley, Steve Chen, and Jawed Karim—created the service in February 2005\nJioSaavn - JioSaavn is an Indian online music streaming service and a digital distributor of Bollywood, English and other regional Indian music across the world. Since it was founded in 2007 as Saavn, the company has acquired rights to over 5 crore (50 million) music tracks in 15 languages\nSongLink - SongLink is a free smart link service that helps you share music with your audience. It's a one-stop-shop for creating smart links for music, podcasts, and other audio content\nLRCLib - A public synced lyric API\nLinux - Linux is a family of open-source Unix-like operating systems based on the Linux kernel, an operating system kernel first released on September 17, 1991, by Linus Torvalds. Linux is typically packaged in a Linux distribution\nAUR - AUR stands for Arch User Repository. It is a community-driven repository for Arch-based Linux distributions users\nFlatpak - Flatpak is a utility for software deployment and package management for Linux\nSponsorBlock - SponsorBlock is an open-source crowdsourced browser extension and open API for skipping sponsor segments in YouTube videos.\nInno Setup - Inno Setup is a free installer for Windows programs by Jordan Russell and Martijn Laan\nF-Droid - F-Droid is an installable catalogue of FOSS (Free and Open Source Software) applications for the Android platform. The client makes it easy to browse, install, and keep track of updates on your device\nLastFM - Last.fm is a music streaming and discovery platform that helps users discover and share new music. It tracks users' music listening habits across many devices and platforms.\n\nDependencies\n\napp_links - Android App Links, Deep Links, iOs Universal Links and Custom URL schemes handler for Flutter (desktop included).\nargs - Library for defining parsers for parsing raw command-line arguments into a set of options and values using GNU and POSIX style options.\nasync - Utility functions and classes related to the 'dart:async' library.\naudio_service_mpris - audio_service platform interface supporting Media Player Remote Interfacing Specification.\naudio_service - Flutter plugin to play audio in the background while the screen is off.\naudio_session - Sets the iOS audio session category and Android audio attributes for your app, and manages your app's audio focus, mixing and ducking behaviour.\nauto_size_text - Flutter widget that automatically resizes text to fit perfectly within its bounds.\nbonsoir - A Zeroconf library that allows you to discover network services and to broadcast your own. Based on Apple Bonjour and Android NSD.\nbuild_runner - A build system for Dart code generation and modular compilation.\nbuttons_tabbar - A Flutter package that implements a TabBar where each label is a toggle button.\ncached_network_image - Flutter library to load and cache network images. Can also be used with placeholder and error widgets.\ncatcher_2 - Plugin for error catching which provides multiple handlers for dealing with errors when they are not caught by the developer.\ncollection - Collections and utilities functions and classes related to collections.\ncrypto - Implementations of SHA, MD5, and HMAC cryptographic functions.\ncurved_navigation_bar - Stunning Animating Curved Shape Navigation Bar. Adjustable color, background color, animation curve, animation duration.\ncustom_lint - Lint rules are a powerful way to improve the maintainability of a project. Custom Lint allows package authors and developers to easily write custom lint rules.\ndart_discord_rpc - Discord Rich Presence for Flutter & Dart apps & games.\ndbus - A native Dart implementation of the D-Bus message bus client. This package allows Dart applications to directly access services on the Linux desktop.\ndevice_info_plus - Flutter plugin providing detailed information about the device (make, model, etc.), and Android or iOS version the app is running on.\ndio - A powerful HTTP networking package,supports Interceptors,Aborting and canceling a request,Custom adapters, Transformers, etc.\ndisable_battery_optimization - Flutter plugin to check and disable battery optimizations. Also shows custom steps to disable the optimizations in devices like mi, xiaomi, samsung, oppo, huawei, oneplus etc\ndraggable_scrollbar - A scrollbar that can be dragged for quickly navigation through a vertical list. Additional option is showing label next to scrollthumb with information about current item.\nduration - Utilities to make working with 'Duration's easier. Formats duration in human readable form and also parses duration in human readable form to Dart's Duration.\nenvied_generator - Generator for the Envied package. See https://pub.dev/packages/envied.\nenvied - Explicitly reads environment variables into a dart file from a .env file for more security and faster start up times.\nfile_picker - A package that allows you to use a native file explorer to pick single or multiple absolute file paths, with extension filtering support.\nfile_selector - Flutter plugin for opening and saving files, or selecting directories, using native file selection UI.\nfluentui_system_icons - Fluent UI System Icons are a collection of familiar, friendly and modern icons from Microsoft.\nflutter_broadcasts - A plugin for sending and receiving broadcasts with Android intents and iOS notifications.\nflutter_cache_manager - Generic cache manager for flutter. Saves web files on the storages of the device and saves the cache info using sqflite.\nflutter_displaymode - A Flutter plugin to set display mode (resolution, refresh rate) on Android platform. Allows to enable high refresh rate on supported devices.\nflutter_feather_icons - Feather is a collection of simply beautiful open source icons. Each icon is designed on a 24x24 grid with an emphasis on simplicity, consistency and usability.\nflutter_gen_runner - The Flutter code generator for your assets, fonts, colors, … — Get rid of all String-based APIs.\nflutter_hooks - A flutter implementation of React hooks. It adds a new kind of widget with enhanced code reuse.\nflutter_inappwebview - A Flutter plugin that allows you to add an inline webview, to use an headless webview, and to open an in-app browser window.\nflutter_launcher_icons - A package which simplifies the task of updating your Flutter app's launcher icon.\nflutter_lints - Recommended lints for Flutter apps, packages, and plugins to encourage good coding practices.\nflutter_native_splash - Customize Flutter's default white native splash screen with background color and splash image. Supports dark mode, full screen, and more.\nflutter_riverpod - A reactive caching and data-binding framework. Riverpod makes working with asynchronous code a breeze.\nflutter_secure_storage - Flutter Secure Storage provides API to store data in secure storage. Keychain is used in iOS, KeyStore based solution is used in Android.\nflutter_sharing_intent - A flutter plugin that allow flutter apps to receive photos, videos, text, urls or any other file types from another app.\nflutter_svg - An SVG rendering and widget library for Flutter, which allows painting and displaying Scalable Vector Graphics 1.1 files.\nform_validator - Simplest form validation library for flutter's form field widgets\nfreezed_annotation - Annotations for the freezed code-generator. This package does nothing without freezed too.\nfreezed - Code generation for immutable classes that has a simple syntax/API without compromising on the features.\nfuzzywuzzy - An implementation of the popular fuzzywuzzy package in Dart, to suit all your fuzzy string matching/searching needs!\ngap - Flutter widgets for easily adding gaps inside Flex widgets such as Columns and Rows or scrolling views.\ngo_router - A declarative router for Flutter based on Navigation 2 supporting deep linking, data-driven routes and more\ngoogle_fonts - A Flutter package to use fonts from fonts.google.com. Supports HTTP fetching, caching, and asset bundling.\nhive_flutter - Extension for Hive. Makes it easier to use Hive in Flutter apps.\nhive_generator - Extension for Hive. Automatically generates TypeAdapters to store any class.\nhive - Lightweight and blazing fast key-value database written in pure Dart. Strongly encrypted using AES-256.\nhooks_riverpod - A reactive caching and data-binding framework. Riverpod makes working with asynchronous code a breeze.\nhtml_unescape - A small library for un-escaping HTML. Supports all Named Character References, Decimal Character References and Hexadecimal Character References.\nhtml - APIs for parsing and manipulating HTML content outside the browser.\nhttp - A composable, multi-platform, Future-based API for HTTP requests.\nimage_picker - Flutter plugin for selecting images from the Android and iOS image library, and taking new pictures with the camera.\nintl - Contains code to deal with internationalized/localized messages, date and number formatting and parsing, bi-directional text, and other internationalization issues.\nintroduction_screen - Introduction/Onboarding package for flutter app with some customizations possibilities\nio - Utilities for the Dart VM Runtime including support for ANSI colors, file copying, and standard exit code values.\njiosaavn - Unofficial API client for jiosaavn.com\njson_annotation - Classes and helper functions that support JSON code generation via the json_serializable package.\njson_serializable - Automatically generate code for converting to and from JSON by annotating Dart classes.\nlocal_notifier - This plugin allows Flutter desktop apps to displaying local notifications.\nlogger - Small, easy to use and extensible logger which prints beautiful logs.\nlrc - A Dart-only package that creates, parses, and handles LRC, which is a format that stores song lyrics.\nmedia_kit_libs_audio - package:media_kit audio (only) playback native libraries for all platforms.\nmedia_kit - A cross-platform video player & audio player for Flutter & Dart. Performant, stable, feature-proof & modular.\nmetadata_god - Plugin for retrieving and writing audio tags/metadata from audio files\nmime - Utilities for handling media (MIME) types, including determining a type from a file extension and file contents.\npackage_info_plus - Flutter plugin for querying information about the application package, such as CFBundleVersion on iOS or versionCode on Android.\npalette_generator - Flutter package for generating palette colors from a source image.\npath_provider - Flutter plugin for getting commonly used locations on host platform file systems, such as the temp and app data directories.\npath - A string-based path manipulation library. All of the path operations you know and love, with solid support for Windows, POSIX (Linux and Mac OS X), and the web.\npermission_handler - Permission plugin for Flutter. This plugin provides a cross-platform (iOS, Android) API to request and check permissions.\npiped_client - API Client for piped.video\npopover - A popover is a transient view that appears above other content onscreen when you tap a control or in an area.\nprocess_run - Process run helpers for Linux/Win/Mac and which like feature for finding executables.\npub_api_client - An API Client for Pub to interact with public package information.\npubspec_parse - Simple package for parsing pubspec.yaml files with a type-safe API and rich error reporting.\nriverpod_lint - Riverpod_lint is a developer tool for users of Riverpod, designed to help stop common issues and simplify repetitive tasks.\nscrobblenaut - A deadly simple LastFM API Wrapper for Dart. So deadly simple that it's gonna hit the mark.\nscroll_to_index - Scroll to a specific child of any scrollable widget in Flutter\nshared_preferences - Flutter plugin for reading and writing simple key-value pairs. Wraps NSUserDefaults on iOS and SharedPreferences on Android.\nshelf_router - A convenient request router for the shelf web-framework, with support for URL-parameters, nested routers and routers generated from source annotations.\nshelf_web_socket - A shelf handler that wires up a listener for every connection.\nshelf - A model for web server middleware that encourages composition and easy reuse.\nsidebarx - flutter multiplatform navigation sidebar / side navigationbar / drawer widget\nsimple_icons - The Simple Icon pack available as Flutter Icons. Provides over 1500 Free SVG icons for popular brands.\nskeleton_text - A package that provides an easy way to add skeleton text loading animation in Flutter project. This project is a part of 101Loop community.\nskeletonizer - Converts already built widgets into skeleton loaders with no extra effort.\nsliver_tools - A set of useful sliver tools that are missing from the flutter framework\nsmtc_windows - Windows SystemMediaTransportControls implementation for Flutter giving access to Windows OS Media Control applet.\nspotify - An incomplete dart library for interfacing with the Spotify Web API.\nstroke_text - A Simple Flutter plugin for applying stroke (border) style to a text widget\nsystem_theme - A plugin to get the current system theme info. Supports Android, Web, Windows, Linux and macOS\ntimezone - Time zone database and time zone aware DateTime.\ntitlebar_buttons - A package which provides most of the titlebar buttons from windows, linux and macos.\ntray_manager - This plugin allows Flutter desktop apps to defines system tray.\nurl_launcher - Flutter plugin for launching a URL. Supports web, phone, SMS, and email schemes.\nuuid - RFC4122 (v1, v4, v5, v6, v7, v8) UUID Generator and Parser for Dart\nversion - Provides a simple class for parsing and comparing semantic versions as defined by http://semver.org/\nvery_good_infinite_list - A library for easily displaying paginated data, created by Very Good Ventures. Great for activity feeds, news feeds, and more.\nvisibility_detector - A widget that detects the visibility of its child and notifies a callback.\nweb_socket_channel - StreamChannel wrappers for WebSockets. Provides a cross-platform WebSocketChannel API, a cross-platform implementation of that API that communicates over an underlying StreamChannel.\nwikipedia_api - Wikipedia API for dart and flutter\nwin32_registry - A package that provides a friendly Dart API for accessing the Windows Registry.\nwindow_manager - This plugin allows Flutter desktop apps to resizing and repositioning the window.\nxml - A lightweight library for parsing, traversing, querying, transforming and building XML documents.\nyoutube_explode_dart - A port in dart of the youtube explode library. Supports several API functions without the need of Youtube API Key.\n\n\n© Copyright Spotube 2024"
}
